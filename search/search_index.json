{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#_1","title":"Home","text":"Lesson Overview 1. Download and verify data Downloading data with <code>wget</code>/<code>curl</code> and check the transferred data\u2019s integrity with check\u2010sums 2. Streams, Redirection and Pipe Combining pipes and redirection, Using \"Exit\" statuses 3. Inspecting and Manipulating Text Data with UNIX Tools - Part 1 Inspect file/s with utilities such as <code>head</code>,<code>less</code>. Extracting and formatting tabular data. Magical <code>grep</code>. 4. Inspecting and Manipulating Text Data with UNIX Tools - Part 2 Substitute matching patterns with <code>sed</code>. Text processing with <code>awk</code> and <code>bioawk</code> 5. Automating File-Processing with find and xargs Search files by pattern with <code>find</code> and use <code>xargs</code> to execute a command for those objects matching the pattern 6. Puzzles \ud83e\udde9 Can you use shell scripts to solve these \"real\" life challenges in molecular biology ? 7. Supplementary - 1 Recap - Unix , Linux and Unix shell 8. Supplementary - 2 Recap - Shell basics and commands 9. Supplementary - 3 Escaping, Special Characters <p>Attribution Notice</p> <ul> <li>This workshop material is heavily inspired by : <ol> <li>Buffalo, V (2015). Bioinformatics Data Skills.O'Reilly Media, Inc</li> <li>The Carpentries. The Unix Shell . https://swcarpentry.github.io/shell-novice/</li> <li>The Carpentries. Introduction to Command Line for Genomics. https://datacarpentry.org/shell-genomics/</li> <li>Rosalind Project. https://rosalind.info/about/</li> </ol> </li> </ul> <p>License</p> <p>Genomics Aotearoa / New Zealand eScience Infrastructure \"Intermediate Shell for Bioinformatics\" is licensed under the GNU General Public License v3.0, 29 June 2007 . (Follow this link for more information)</p> <p>Setup</p> <ul> <li> <p>If possible, we do recommend using the Remote option over Local  ( Especially for Windows hosts). This will eliminate  the need to install any additional applications</p> </li> <li> <p>Remote option will require an existing NeSI Account</p> </li> </ul>"},{"location":"#remote","title":"Remote","text":"Log into NeSI Mahuika Jupyter Service - Not required if the workshop is running on OpenOnDemand based Training environment <ol> <li>Follow https://jupyter.nesi.org.nz/hub/login</li> <li><p>Enter NeSI username, HPC password and 6 digit second factor token</p></li> <li><p>Choose server options as below &gt;&gt;make sure to choose the correct project code <code>nesi02659</code>, number of CPUs <code>CPUs=2</code>, memory <code>4 GB</code> prior to pressing   button. <p></p>"},{"location":"#local","title":"Local","text":"Local host setup - Windows, MacOS &amp; Linux Windows HostsMacOSLinux <ul> <li>Install either </li> <li>Git for Windows from https://git-scm.com/download/win OR</li> <li>MobaXterm Home (Portable or Installer edition) from https://mobaxterm.mobatek.net/download-home-edition.html<ul> <li>Portable edition does not require administrative privileges </li> </ul> </li> </ul> <ul> <li>Native terminal client is sufficient.</li> <li>It might not comes with <code>wget</code> download data via command line (can be installed with <code>$ brew install wget</code>)</li> <li>However, it is not required as we provide a direct link to download data in .zip format </li> </ul> <ul> <li>Native terminal client is sufficient.</li> </ul> <p><code>bioawk</code> install on all hosts</p> <p>One of the tools used in this workshop is <code>bioawk</code> which is not a native Linu/UNIX utility. Installing it on MacOS and Linux can be done with <code>$ brew install bioawk</code> &amp; <code>$ sudo apt install bioawk</code>, respectively. Windows hosts might have to do it via <code>conda</code> according to these instructions. However, this will require a prior install of Anaconda Or Miniconda </p>"},{"location":"1_overview/","title":"Overview","text":"<p>Overall objective of the workshop</p> <ul> <li>Introduce few \"intermediate\" level linux tools, tips and tricks which can be incorporated to routine tasks while dealing with Biological data formats (or any file format)</li> <li>It may feels like the workshops lack a story (apologies in advance) but, <ul> <li>Methods taught here will be used in upcoming \"Advanced Shell for Bioinformatics\" to design \"shell\" based workflows (both serial and parallel)</li> <li>And some tools are already being used in other advanced level workshops where this material can be used as a reference</li> </ul> </li> </ul> <p></p> <p>NOTE - some commands will have the following sub-header \"Why is this useful\" to list few instances where a  specific tool/command can be used in a Bioinformatics workflow</p> Why is this useful ?"},{"location":"2_download_data/","title":"1. Download &amp; Verify Data","text":"<p>Lesson Objectives</p> <ul> <li>Inspect data with <code>md5</code> (checksum verification)</li> </ul> Download with <code>wget</code> from a terminal client (optional)  - Not required for  NeSI Training platform <p><pre><code>wget -c https://github.com/GenomicsAotearoa/shell-for-bioinformatics/releases/download/v1.0/shell4b_data.tar.gz\n</code></pre> OR</p> <pre><code>wget -c https://github.com/GenomicsAotearoa/shell-for-bioinformatics/releases/download/v1.0/shell4b_data.zip\n</code></pre> Download via Web Browser (use locally) <ul> <li>Data can be downloaded directly from this link which will download shell4b_data.tar.gz to Downloads directory</li> <li>If the above link fails, try this alternative .zip</li> </ul> <p></p> Recap - Decompress downloaded tar.gz OR .zip <p>A TAR.GZ file is a combination of two different packaging algorithms. The first is tar, short for tape archive. It\u2019s an old utility invented mainly for accurate data transfer to devices without their own file systems. A tar file (or tarball) contains files in a sequential format, along with metadata about the directory structure and other technical parameters.</p> <p>It is useful to note that tar doesn\u2019t compress the files in question, only packages them. Indeed, sometimes the resulting tarball can be of greater size due to padding. That\u2019s where Gzip comes in. Gzip (denoted by a .gz file extension) is a compressed file format used to archive data to take up smaller space. Gzip uses the same compression algorithm as the more commonly known zip but can only be used on a single file. In short, Gzip compresses all the individual files and tar packages them in a single archive.</p> <p>Decompressing the tar.gz file can be done  with the built-in <code>tar</code> utility</p> <pre><code>tar -xvzf shell4b_data.tar.gz\n</code></pre> <ul> <li><code>x</code>: Extract an archive.</li> <li><code>z</code>: Compress the archive with gzip.</li> <li><code>v</code>: Display progress in the terminal while creating the archive, also known as \u201cverbose\u201d mode. The <code>v</code>is always optional in these commands, but it\u2019s helpful.</li> <li><code>f</code>: Allows you to specify the filename of the archive.</li> </ul> <p>ZIP files can store multiple files using different compression techniques while at the same time supports storing a file without any compression. Each file is stored/compressed individually which helps to extract them, or add new ones, without applying compression or decompression to the entire archive.</p> <p>Each file in a ZIP archive is represented as an individual entry consisting of a Local File Header followed by the compressed file data. The Directory at the end of the archive holds the references to all these file entries. ZIP file readers should avoid reading the local file headers and all types of file listing should be read from the Directory. This Directory is the only source for valid file entries in the archive as files can be appended towards the end of the archive as well. That is why if a reader reads local headers of a ZIP archive from the beginning, it may read invalid (deleted) entries as well those are not part of the Directory being deleted from archive.</p> <p>The order of the file entries in the central directory need not coincide with the order of file entries in the archive.</p> <p>Decompressing .zip files can be done with <code>unzip</code> command</p> <pre><code>unzip -v shell4b_data.zip\n</code></pre> <ul> <li><code>v</code>: Display progress in the terminal while creating the archive, also known as \u201cverbose\u201d mode. The <code>v</code>is always optional in these commands, but it\u2019s helpful.</li> </ul> <p></p>"},{"location":"2_download_data/#data-integrity","title":"Data Integrity","text":"<p>Data we download is the starting point of all future analyses and conclusions. Therefore it\u2019s important to explicitly check the transferred data\u2019s integrity with check\u2010sums. Checksums are very compressed summaries of data, computed in a way that even if just one bit of the data is changed the checksum will be different. As such, data integrity checks are also helpful in keeping track of data versions. Checksums facilitate reproducibility, as we can link a particular analysis and set of results to an exact version of data summarized by the data\u2019s checksum value.</p> <p>SHA and MD5 Checksums</p> <p>Two most common checksum algorithms are MD5 and SHA (Specifically referring to SHA256)</p> <ul> <li> <p>MD5 : cryptographic hash function algorithm that takes the message as input of any length and changes it into a fixed-length message of 16 bytes. MD5 algorithm stands for the message-digest algorithm.</p> </li> <li> <p>SHA  : SHA-256 is a more secure and newer cryptographic hash function that was launched in 2000 as a new version of SHA functions and was adopted as Federal Information Processing Standards (FIPS) in 2002. It is allowed to use a hash generator tool to produce a SHA256 hash for any string or input value. Also, it generates 256 hash values, and the internal state size is 256 bit and the original message size is up to 264-1 bits.</p> </li> </ul> <p>To create checksums, we can pass arbitrary strings to the program <code>md5sum</code> (or <code>sha256sum</code>) through standard in</p> <pre><code>echo \"shell for Bioinformatics\" | md5sum\necho \"shell for BioInformatics\" | md5sum\n</code></pre> <pre><code>198638c380be53bf3f6ff70d5626ae44  -\nafa4dbcc56b540e24558085fdc10342f  -\n</code></pre> <p>Checksums are reported in hexadecimal format, where each digit can be one of 16 characters: digits 0 through 9, and the letters a, b, c, d, e, and f. The trailing dash indicates this is the MD5 checksum of input from standard in. Checksums with file input can be done with <code>md5usm filename</code> .i.e.</p> <p>code</p> <p>NeSI training environment terminal will open with with working directory already being set to <code>shell4b_data</code>. If not, change directory with,</p> <p><pre><code>cd shell4b_data\n</code></pre> * Use <code>md5sum</code> to print the hash key</p> <pre><code>md5sum tb1.fasta\n</code></pre> <pre><code>f44dca62012017196b545a2dd2d2906d  tb1.fasta\n</code></pre> <p>Because it can get rather tedious to check each checksum individually, both <code>md5sum</code> and <code>sha256sum</code> has a convenient solution: Let's say that we want to verify the checksums for all of the .fasta files in the current directory.  (*This can be done by creating and validating against a file containing the checksums of all the .fasta files.</p> <ul> <li><code>fasta_checksums.md5</code> contains the checksums for three .fasta files which were verified prior to uploading to github repo. (This was generated by using the command <code>md5sum *.fasta &gt; fasta_checksums.md5</code>)</li> <li>Run the following command and see whether all the .fasta files pass the verification<ul> <li><code>-c</code> option represent check <pre><code>md5sum -c fasta_checksums.md5\n</code></pre></li> </ul> </li> </ul> <p>Applications will not trigger clear error messages for corrupted data</p> <p>The following is an error message recorded on the log for a failed <code>bedtools genomecov</code> process ran on NeSI Mahuika cluster <pre><code>terminate called after throwing an instance of 'std::bad_alloc'\nwhat(): std::bad_alloc\n</code></pre></p> <ul> <li>Doing a Google search for <code>std::bad_alloc</code> will take us to this official reference documentation</li> <li>Expand the search a bit more with <code>BedTools std::bad_alloc</code> which will return this reported issue on Biostars as the first result.</li> </ul> <p><code>std::bad_alloc</code> is the C++ error code for when a new operator tries to allocate something, but fails. As C++ cannot dynamically   allocate memory, the lack of memory is the most common cause.</p> <p>So, it's quite difficult avoid the temptation to throw more memory at this process and re-run it. In fact, it was re-run with 0.5TB of memory as a sanity check which triggered the same error.</p> <p>As it turned out, .bed file was corrupted .  \ud83d\udd75\ufe0f\u200d\u2640\ufe0f</p>"},{"location":"3_streams_red_pipe/","title":"2. Streams, Redirection and Pipe","text":"<p>Lesson Objectives</p> <ul> <li>To be able to redirect streams of data in Unix.</li> <li>Solve problems by piping several Unix commands.</li> <li>Command substitution </li> </ul> <p>Bioinformatics data is often text-based and large. This is why Unix\u2019s philosophy of handling text streams is useful in bioinformatics: text streams allow us to do processing on a stream of data rather than holding it all in memory. Handling and redirecting the streams of data is an essential skill in Unix.</p> <p></p> <p>By default, both standard error and standard output of most unix programs go to your terminal screen. We can change this behavior (redirect the streams to a file) by using <code>&gt;</code> or <code>&gt;&gt;</code> operators. The operator <code>&gt;</code> redirects standard output to a file and overwrites any existing contents of the file, whereas <code>&gt;&gt;</code> appends to the file. If there isn\u2019t an existing file, both operators will create it before redirecting output to it. </p>"},{"location":"3_streams_red_pipe/#output-redirection","title":"Output redirection","text":"<p>The <code>shell4b_data</code> directory contains the following fasta files:</p> <pre><code>tb1.fasta\ntb1-protein.fasta\ntga1-protein.fasta\n</code></pre> <p>We can use the <code>cat</code> command to view these files either one at a time:</p> Recap - <code>cat</code> command to view the content of a file <p>code</p> <pre><code>cat tb1-protein.fasta\n</code></pre> output <pre><code>&gt;teosinte-branched-1 protein\nLGVPSVKHMFPFCDSSSPMDLPLYQQLQLSPSSPKTDQSSSFYCYPCSPP\nFAAADASFPLSYQIGSAAAADATPPQAVINSPDLPVQALMDHAPAPATEL\nGACASGAEGSGASLDRAAAAARKDRHSKICTAGGMRDRRMRLSLDVARKF\nFALQDMLGFDKASKTVQWLLNTSKSAIQEIMADDASSECVEDGSSSLSVD\nGKHNPAEQLGGGGDQKPKGNCRGEGKKPAKASKAAATPKPPRKSANNAHQ\nVPDKETRAKARERARERTKEKHRMRWVKLASAIDVEAAAASVPSDRPSSN\nNLSHHSSLSMNMPCAAA\n</code></pre> <p>code</p> <pre><code>cat tga1-protein.fasta \n</code></pre> output <pre><code>&gt;teosinte-glume-architecture-1 protein\nDSDCALSLLSAPANSSGIDVSRMVRPTEHVPMAQQPVVPGLQFGSASWFP\nRPQASTGGSFVPSCPAAVEGEQQLNAVLGPNDSEVSMNYGGMFHVGGGSG\nGGEGSSDGGT\n</code></pre> <p>OR all at once with <code>cat *.fasta</code></p> <p>We can also redirect the output to create a new file containing the sequence for both proteins:</p> <p>code</p> <pre><code>cat tb1-protein.fasta tga1-protein.fasta &gt; zea-proteins.fasta\n</code></pre> <p>Now we have a new file called <code>zea-proteins.fasta</code>. Let's check the contents:</p> <p>code</p> <pre><code>cat zea-proteins.fasta\n</code></pre> output <pre><code>&gt;teosinte-branched-1 protein\nLGVPSVKHMFPFCDSSSPMDLPLYQQLQLSPSSPKTDQSSSFYCYPCSPP\nFAAADASFPLSYQIGSAAAADATPPQAVINSPDLPVQALMDHAPAPATEL\nGACASGAEGSGASLDRAAAAARKDRHSKICTAGGMRDRRMRLSLDVARKF\nFALQDMLGFDKASKTVQWLLNTSKSAIQEIMADDASSECVEDGSSSLSVD\nGKHNPAEQLGGGGDQKPKGNCRGEGKKPAKASKAAATPKPPRKSANNAHQ\nVPDKETRAKARERARERTKEKHRMRWVKLASAIDVEAAAASVPSDRPSSN\nNLSHHSSLSMNMPCAAA\n&gt;teosinte-glume-architecture-1 protein\nDSDCALSLLSAPANSSGIDVSRMVRPTEHVPMAQQPVVPGLQFGSASWFP\nRPQASTGGSFVPSCPAAVEGEQQLNAVLGPNDSEVSMNYGGMFHVGGGSG\nGGEGSSDGGT\n</code></pre> <p>Capturing error messages</p> <p>code</p> <pre><code>cat tb1-protein.fasta mik.fasta\n</code></pre> <pre><code>&gt;teosinte-branched-1 protein\nLGVPSVKHMFPFCDSSSPMDLPLYQQLQLSPSSPKTDQSSSFYCYPCSPP\nFAAADASFPLSYQIGSAAAADATPPQAVINSPDLPVQALMDHAPAPATEL\nGACASGAEGSGASLDRAAAAARKDRHSKICTAGGMRDRRMRLSLDVARKF\nFALQDMLGFDKASKTVQWLLNTSKSAIQEIMADDASSECVEDGSSSLSVD\nGKHNPAEQLGGGGDQKPKGNCRGEGKKPAKASKAAATPKPPRKSANNAHQ\nVPDKETRAKARERARERTKEKHRMRWVKLASAIDVEAAAASVPSDRPSSN\nNLSHHSSLSMNMPCAAA\ncat: mik.fasta: No such file or directory\n</code></pre> <p>There are two different types of output there: standard output (the contents of the <code>tb1-protein.fasta</code> file) and standard error (the error message relating to the missing <code>mik.fasta</code> file). If we use the <code>&gt;</code> operator to redirect the output, the standard output is captured, but the standard error is not - it is still printed to the screen.  Let's check:</p> <p>code</p> <pre><code>cat tb1-protein.fasta mik.fasta &gt; test.fasta\n\n  cat: mik.fasta: No such file or directory\n</code></pre> <p>The new file has been created and contains the standard output (contents of the file <code>tb1-protein.fasta</code>):</p> <p>code</p> <pre><code>cat test.fasta\n</code></pre> output <pre><code>&gt;teosinte-branched-1 protein\nLGVPSVKHMFPFCDSSSPMDLPLYQQLQLSPSSPKTDQSSSFYCYPCSPP\nFAAADASFPLSYQIGSAAAADATPPQAVINSPDLPVQALMDHAPAPATEL\nGACASGAEGSGASLDRAAAAARKDRHSKICTAGGMRDRRMRLSLDVARKF\nFALQDMLGFDKASKTVQWLLNTSKSAIQEIMADDASSECVEDGSSSLSVD\nGKHNPAEQLGGGGDQKPKGNCRGEGKKPAKASKAAATPKPPRKSANNAHQ\nVPDKETRAKARERARERTKEKHRMRWVKLASAIDVEAAAASVPSDRPSSN\nNLSHHSSLSMNMPCAAA\n</code></pre> <p>If we want to capture the standard error we use the (slightly unweildy) <code>2&gt;</code> operator:</p> <p>code</p> <pre><code>cat tb1-protein.fasta mik.fasta &gt; test.fasta 2&gt; stderror.txt\n</code></pre> <p>Descriptors</p> <p>File descriptor <code>2</code> represents standard error (other special file descriptors include <code>0</code> for standard input and <code>1</code> for standard output).</p> <p>Check the contents:</p> <p>code</p> <pre><code>cat stderror.txt\n\n  cat: mik.fasta: No such file or directory\n</code></pre> <p>Reminder :<code>&gt;</code> vs <code>&gt;&gt;</code></p> <p>Note that <code>&gt;</code> will overwrite an existing file. We can use <code>&gt;&gt;</code> to add to a file instead of overwriting it:</p> <p>code</p> <p><pre><code>cat tga1-protein.fasta &gt;&gt; test.fasta\n</code></pre> <pre><code>cat test.fasta \n</code></pre></p> output <pre><code>&gt;teosinte-branched-1 protein\nLGVPSVKHMFPFCDSSSPMDLPLYQQLQLSPSSPKTDQSSSFYCYPCSPP\nFAAADASFPLSYQIGSAAAADATPPQAVINSPDLPVQALMDHAPAPATEL\nGACASGAEGSGASLDRAAAAARKDRHSKICTAGGMRDRRMRLSLDVARKF\nFALQDMLGFDKASKTVQWLLNTSKSAIQEIMADDASSECVEDGSSSLSVD\nGKHNPAEQLGGGGDQKPKGNCRGEGKKPAKASKAAATPKPPRKSANNAHQ\nVPDKETRAKARERARERTKEKHRMRWVKLASAIDVEAAAASVPSDRPSSN\nNLSHHSSLSMNMPCAAA\n&gt;teosinte-glume-architecture-1 protein\nDSDCALSLLSAPANSSGIDVSRMVRPTEHVPMAQQPVVPGLQFGSASWFP\nRPQASTGGSFVPSCPAAVEGEQQLNAVLGPNDSEVSMNYGGMFHVGGGSG\nGGEGSSDGGT\n</code></pre>"},{"location":"3_streams_red_pipe/#the-unix-pipe","title":"The Unix pipe","text":"<p>The pipe operator (<code>|</code>) passes the output from one command to another command as input.  The following is an example of using a pipe with the <code>grep</code> command.</p> <p>Steps:</p> <ol> <li>Remove the header information for the sequence (line starts with \"&gt;\")</li> <li>Highlight any characters in the sequence that are not A, T, C or G.</li> </ol> <p>We will use grep to carry out the first step, and then use the pipe operator to pass the output to a second grep command to carry out the second step.</p> <p>Here is the full command:</p> <p>code</p> <pre><code>grep -v \"^&gt;\" tb1.fasta | grep --color -i \"[^ATCG]\"\n</code></pre> <p>Let's see what each piece does</p> <p><code>grep -v \"^&gt;\" tb1.fasta</code></p> <ul> <li><code>-v</code>: Inverts the match, selecting non-matching lines</li> <li><code>\"^&gt;\"</code>: The pattern to match. ^ means start of line, &gt; is the literal character</li> </ul> <p><code>grep --color -i \"[^ATCG]\"</code></p> <p>There are a few things going on here:</p> <ul> <li><code>--color</code>: tells <code>grep</code> to highlight any matches</li> <li><code>-i</code>: tells <code>grep</code> to ignore the case (i.e., will match lower or upper case)</li> <li><code>[^ATCG]</code>: when <code>^</code> is used inside square brackets it has a different function - inverts the pattern, so that <code>grep</code> finds any letters that are not A, T, C or G. In other words,  <code>[^...]</code> means \"any character not in this set\"</li> </ul> Why is this useful <ul> <li>Identifying non-standard nucleotides in a DNA sequence</li> <li>Spotting potential sequencing errors</li> <li>Quality control of DNA sequence data</li> </ul> <p>Let's run the code:</p> <p>code</p> <pre><code>grep -v \"^&gt;\" tb1.fasta | grep --color -i \"[^ATCG]\"\n</code></pre> Output <p>CCCCAAAGACGGACCAATCCAGCAGCTTCTACTGCTAYCCATGCTCCCCTCCCTTCGCCGCCGCCGACGC</p>"},{"location":"3_streams_red_pipe/#combining-pipes-and-redirection","title":"Combining pipes and redirection","text":"<p>redirect the standard output of above <code>grep..</code> command to <code>non-atcg.txt</code></p> <pre><code>grep -v \"^&gt;\" tb1.fasta | grep --color -i \"[^ATCG]\" &gt; non-atcg.txt\n</code></pre> <p>code</p> <pre><code>cat non-atcg.txt \n</code></pre> <p>since we are redirecting to a text file, the <code>--color</code> by itself will not record the colour information. We can achieve this by invoking <code>always</code> flag for <code>--color</code>.i.e..</p> <p>code</p> <pre><code>grep -v \"^&gt;\" tb1.fasta | grep --color=always -i \"[^ATCG]\" &gt; non-atcg.txt\n</code></pre>"},{"location":"3_streams_red_pipe/#using-tee-to-capture-intermediate-outputs","title":"Using tee to capture intermediate outputs","text":"<p>code</p> <pre><code>grep -v \"^&gt;\" tb1.fasta | tee intermediate-out.txt | grep --color=always -i \"[^ATCG]\" &gt; non-atcg.txt\n</code></pre> <p>The file <code>intermediate-out.txt</code> will contain the output from <code>grep -v \"^&gt;\" tb1.fasta</code>, but <code>tee</code> also passes that output through the pipe to the next <code>grep</code> command.</p> Preview - This is to be covered in \"Advanced Shell for Bioinformatics\"Pipes and Chains and Long running processes  : Exit Status (Programmatically Tell Whether Your Command Worked) <p>How do you know when they complete? How do you know if they successfully finished without an error? Unix programs exit with an exit status, which indicates whether a program terminated without a problem or with an error. By Unix standards, an exit status of <code>0</code> indicates the process ran successfully, and any nonzero status indicates some sort of error has occurred (and hopefully the program prints an understandable error message, too). The exit status isn\u2019t printed to the terminal, but your shell will set its value to a shell variable named   <code>$?</code>. We can use the <code>echo</code> command to look at this variable\u2019s value after running a command:</p> <p><pre><code>program input.txt &gt; results.txt; echo $?\n</code></pre> Exit statuses are useful because they allow us to programmatically chain commands together in the shell. A subsequent command in a chain is run conditionally on the last command\u2019s exit status. The shell provides two operators that implement this: one operator that runs the subsequent command only if the first command completed successfully (<code>&amp;&amp;</code>), and one operator that runs the next command only if the first completed unsuccessfully (<code>||</code>).</p> <p>For example, the sequence <code>program1 input.txt &gt; intermediate-results.txt &amp;&amp; program2 intermediate-results.txt &gt; results.txt</code> will execute the second command only if previous commands have completed with a successful zero exit status.</p> <p>By contrast, <code>program1 input.txt &gt; intermediate-results.txt || echo \"warning: an error occurred\"</code> will print the message if error has occurred.</p> <p>When a script ends with an exit that has no parameter, the exit status of the script is the exit status of the last command executed in the script (previous to the exit).</p> <p>Exit Status : using <code>&amp;&amp;</code> and <code>||</code></p> <p>To test your understanding of <code>&amp;&amp;</code> and <code>||</code>, we\u2019ll use two Unix commands that do nothing but return either exit success (true) or exit failure (false). Predict and check the outcome of the following commands:</p> <pre><code>true\necho $?\nfalse\necho $?\ntrue &amp;&amp; echo \"first command was a success\"\ntrue || echo \"first command was not a success\" \nfalse || echo \"first command was not a success\"\nfalse &amp;&amp; echo \"first command was a success\"\n</code></pre> <p>hint</p> <p>The <code>$?</code> variable represents the exit status of the previous command.</p> Answer <pre><code>~$ true \n~$ echo $?\n0\n~$ false\n~$ echo $?\n1\n~$ true &amp;&amp; echo \"first command was a success\"\nfirst command was a success\n\n~$ true || echo \"first command was not a success\"\n~$ false || echo \"first command was not a success\"\nfirst command was not a success\n\n~$ false &amp;&amp; echo \"first command was a success\"\n~$ \n</code></pre>"},{"location":"3_streams_red_pipe/#command-substitution","title":"Command Substitution","text":"<p>Unix users like to have the Unix shell do the work for them. This is why shell expansions like wildcards and brace expansion exist. Another type of useful shell expansion is command substitution. Command substitution runs a Unix command inline and returns the output as a string that can be used in another command. This opens up a lot of useful possibilities. For example, if you want to include the results from executing a command into a text, you can type:</p> <p>Which is better ?</p> <p><pre><code>grep -c '^@' SRR097977.fastq\n</code></pre> OR</p> <pre><code>echo \"There are $(grep -c '^@' SRR097977.fastq) entries in my FASTA file.\"\n</code></pre> <ul> <li><code>echo</code>: This is a command that prints text to the standard output.</li> <li>The text in quotes is what will be printed, with a substitution: \"There are ... entries in my FASTA file.\"</li> <li><code>$(...)</code>: This is command substitution. It runs the command inside the parentheses and replaces itself with the output of that command.</li> <li> <p><code>grep -c '^@' SRR097977.fastq</code>: This is the command inside the substitution:</p> <ul> <li><code>-c</code>: An option that tells grep to count matching lines instead of printing them</li> <li><code>'^@'</code>: The pattern to search for. In this case, it's looking for lines that start with '@'</li> </ul> </li> <li> <p>So, this command will:</p> <ul> <li>Count how many lines in SRR097977.fastq start with '@'</li> <li>Substitute that number into the echo statement</li> <li>Print the resulting message!</li> </ul> </li> </ul> <p>Another example of using command substitution would be creating dated directories:</p> <p>code</p> <pre><code>mkdir results-$(date +%F)\n</code></pre> <ul> <li><code>%F</code> - full date; same as <code>%Y-%m-%d</code></li> </ul>"},{"location":"4_inspectmanipluate/","title":"3. Inspecting and Manipulating Text Data with Unix Tools - Part 1","text":"<p>Lesson Objectives</p> <ul> <li>Inspect file/s with utilities such as <code>head</code>,<code>less</code>. </li> <li>Extracting and formatting tabular data. </li> <li>Magical <code>grep</code></li> <li>use <code>sort</code>, <code>uniq</code>, <code>join</code> to manipulate the one or multiple files at once</li> </ul> <p></p> <p>Many formats in bioinformatics are simple tabular plain-text files delimited by a character. The most common tabular plain-text file format used in bioinformatics is tab-delimited. Bioinformatics evolved to favor tab-delimited formats because of the convenience of working with these files using Unix tools.</p> <p>Tabular Plain-Text Data Formats</p> <p>Tabular plain-text data formats are used extensively in computing. The basic format is incredibly simple: each row (also known as a record) is kept on its own line, and each column (also known as a field) is separated by some delimiter. There are three flavors you will encounter: tab-delimited, comma-separated, and variable space-delimited.</p> Recap - Inspecting data with <code>head</code> and <code>tail</code> <p>Although <code>cat</code> command is an easy way for us to open and view the content of a file, it is not very practical to do so for a file with thousands of lines as it will exhaust the shell \"space\". Instead, large files should be inspected first and then manipulated accordingly. The first round of inspection can be done with <code>head</code> and <code>tail</code> command which prints the first 10 lines and the last 10 lines (<code>-n 10</code>) of a a file, respectively. Let's use <code>head</code> and <code>tail</code> to inspect Mus_musculus.GRCm38.75_chr1.bed </p> <p>code</p> <pre><code>head Mus_musculus.GRCm38.75_chr1.bed\n</code></pre> Output <pre><code>1   3054233 3054733\n1   3054233 3054733\n1   3054233 3054733\n1   3102016 3102125\n1   3102016 3102125\n1   3102016 3102125\n1   3205901 3671498\n1   3205901 3216344\n1   3213609 3216344\n1   3205901 3207317\n</code></pre> <p>code</p> <pre><code>tail Mus_musculus.GRCm38.75_chr1.bed \n</code></pre> Output <pre><code>1   195166217   195166390\n1   195165745   195165851\n1   195165748   195165851\n1   195165745   195165747\n1   195228278   195228398\n1   195228278   195228398\n1   195228278   195228398\n1   195240910   195241007\n1   195240910   195241007\n1   195240910   195241007\n</code></pre> <p>Changing the number of lines printed for either of those commands can be done by passing <code>-n &lt;number_of_lines&gt;</code> flag .i.e. Over-ride the <code>-n 10</code> default</p> <p>Try those commands with <code>-n 4</code> to print top 4 lines and bottom 4 lines</p> <p><pre><code>head -n 4 Mus_musculus.GRCm38.75_chr1.bed \n</code></pre> <pre><code>tail -n 4 Mus_musculus.GRCm38.75_chr1.bed \n</code></pre></p> <p> </p> Exercise 4.1 <p>Sometimes it\u2019s useful to see both the beginning and end of a file \u2014 for example, if we have a sorted BED file and we want to see the positions of the first feature and last feature. Can you figure out a way to use both <code>head</code> and <code>tail</code> in a single command to inspect first and last 2 lines of Mus_musculus.GRCm38.75_chr1.bed?</p> solution <pre><code>(head -n 2; tail -n 2) &lt; Mus_musculus.GRCm38.75_chr1.bed\n</code></pre> Exercise 4.2 <p>We can also use tail to remove the header of a file. Normally the <code>-n</code> argument specifies how many of the last lines of a file to include, but if <code>-n</code> is given a number <code>x</code> preceded with a <code>+</code> sign (e.g., <code>+x</code> ), tail will start from the x<sup>th</sup> line. So to chop off a header, we start from the second line with <code>-n+2</code>. Use the <code>seq</code> command to generate a file containing the numbers 1 to 10, and then use the <code>tail</code> command to chop off the first line.</p> solution <p><pre><code>seq 10 &gt; nums.txt\n</code></pre> <pre><code>cat nums.txt\n</code></pre> <pre><code>tail -n+2 nums.txt\n</code></pre></p>"},{"location":"4_inspectmanipluate/#extract-summary-information-with-wc","title":"Extract summary information with <code>wc</code>","text":"<p>The \"wc\" in the <code>wc</code> command which stands for \"word count\" - this command can count the numbers of lines, words, and characters in a file (take a note on the order).</p> <p>code</p> <pre><code>wc Mus_musculus.GRCm38.75_chr1.bed \n\n  81226  243678 1698545 Mus_musculus.GRCm38.75_chr1.bed\n</code></pre> <p>Often, we only need to list the number of lines, which can be done by using the <code>-l</code> flag. It can be used as a sanity check - for example, to make sure an output has the same number of lines as the input, OR to check that a certain file format which depends on another format without losing overall data structure wasn't corrupted or over/under manipulated. </p> <p>Question</p> <p>Count the number of lines in Mus_musculus.GRCm38.75_chr1.bed and Mus_musculus.GRCm38.75_chr1.gtf . Anything out of the ordinary ? </p> <p>Although <code>wc -l</code> is the quickest way to count the number of lines in a file, it is not the most robust as it relies on the very bad assumption that \"data is well formatted\" </p> <p>code</p> <ul> <li> <p>For an example, if we are to create a file with 3 rows of data and then two empty lines by pressing Enter twice, </p> <p>Ctrl+D to end the edits started with <code>cat &gt;</code></p> </li> </ul> <pre><code>cat &gt; foo_wc.bed\n</code></pre> <pre><code>  1 100\n  2 200\n  3 300\n[Press Enter]\n[Press Enter]\n</code></pre> <p>code</p> <pre><code>wc -l foo_wc.bed \n</code></pre> <pre><code>  5 foo_wc.bed\n</code></pre> <p>This is a good place to bring in <code>grep</code> again which can be used to count the number of lines while excluding white-spaces (spaces, tabs (<code>t</code>) or newlines (<code>n</code>))</p> <p>code</p> <pre><code>grep -c \"[^ \\n\\t]\" foo_wc.bed \n</code></pre> Output <pre><code>  3\n</code></pre> <ul> <li><code>-c</code>: This option tells grep to output only a count of matching lines instead of the lines themselves.</li> <li><code>\"[^ \\n\\t]\"</code>: This is a regular expression pattern:</li> <li><code>[ ]</code>: Defines a character set<ul> <li>When you place characters inside square brackets, you are specifying a set of characters, any one of which can match at that position in the string.</li> <li>For example, <code>[abc]</code> matches any one of the characters <code>a</code>, <code>b</code>, or <code>c</code>.</li> </ul> </li> <li><code>^</code>: Inside the brackets, this means \"not\"</li> <li><code>\\n</code>: Represents a newline character</li> <li><code>\\t</code>: Represents a tab character</li> </ul> <p>So <code>[^ \\n\\t]</code> means \"match any character that is not a space, newline, or tab\"</p>"},{"location":"4_inspectmanipluate/#using-cut-with-column-data-and-formatting-tabular-data-with-column","title":"Using <code>cut</code> with column data and formatting tabular data with <code>column</code>","text":"<p>When working with plain-text tabular data formats like tab-delimited and CSV files, we often need to extract specific columns from the original file or stream. For example, suppose we wanted to extract only the start positions (the second column) of the Mus_musculus.GRCm38.75_chr1.bed file. The simplest way to do this is with <code>cut</code>.</p> <p>code</p> <pre><code>cut -f 2 Mus_musculus.GRCm38.75_chr1.bed | head -n 3\n</code></pre> <pre><code>3054233\n3054233\n3054233\n</code></pre> <p><code>-f</code>  argument is how we specify which columns to keep. It can be used to specify a range as well</p> <p>code</p> <pre><code>cut -f 2-3 Mus_musculus.GRCm38.75_chr1.bed | head -n 3\n</code></pre> <pre><code> 3054233   3054733\n 3054233   3054733\n 3054233   3054733\n</code></pre> <p><code>-f 2-3</code> is an option that tells cut to extract fields 2 and 3. In this context, fields are typically columns in a text file, separated by a delimiter (by default, a tab).</p> <p>Using <code>cut</code>, we can convert our GTF for Mus_musculus.GRCm38.75_chr1.gtf to a three-column tab-delimited file of genomic ranges (e.g., chromosome, start, and end position). We\u2019ll chop off the metadata rows using the grep command covered earlier and then use cut to extract the first, fourth, and fifth columns (chromosome, start, end):</p> <p>code</p> <pre><code>grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | cut -f 1,4,5 | head -n 3\n</code></pre> <pre><code>1  3054233 3054733\n1  3054233 3054733\n1  3054233 3054733\n</code></pre> Explain please <ul> <li><code>-v</code> inverts the match, selecting lines that do not match the pattern</li> <li> <p><code>\"^#\"</code> is a regular expression that matches lines starting with #</p> <ul> <li>In summary, <code>grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf</code> step removes all comment lines (which typically start with #) from the GTF file</li> </ul> </li> <li> <p><code>-f 1,4,5</code> specifies fields (columns) 1, 4, and 5 - In a GTF file, these columns typically represent:</p> <ul> <li>Field 1: Chromosome name</li> <li>Field 4: Start position of the feature</li> <li>Field 5: End position of the feature</li> </ul> </li> </ul> <p>Note that although our three-column file of genomic positions looks like a BED-formatted file, it\u2019s not (due to subtle differences in genomic range formats).</p> <p><code>cut</code> also allows us to specify the column delimiter character. So, if we were to come across a CSV file containing chromosome names, start positions, and end positions, we could select columns from it, too:</p> <p>As you may have noticed when working with tab-delimited files, it\u2019s not always easy to see which elements belong to a particular column. For example:</p> <p>code</p> <pre><code>grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | cut -f 1-8 | head -n 3\n</code></pre> <pre><code>  1    pseudogene  gene    3054233 3054733 .   +   .\n  1    unprocessed_pseudogene  transcript  3054233 3054733 .   +   .\n  1    unprocessed_pseudogene  exon    3054233 3054733 .   +   .\n</code></pre> <p>While tabs are a great delimiter in plain-text data files, our variable width data causes our columns to not stack up well. There\u2019s a fix for this in Unix: program column <code>-t</code> (the <code>-t</code> option tells column to treat data as a table). The column -t command produces neat columns that are much easier to read:</p> <p>code</p> <pre><code>grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | cut -f 1-8 | column -t | head -n 3\n</code></pre> <pre><code>   1  pseudogene                          gene         3054233    3054733    .  +  .\n   1  unprocessed_pseudogene              transcript   3054233    3054733    .  +  .\n   1  unprocessed_pseudogene              exon         3054233    3054733    .  +  .\n</code></pre> <p>Note that you should only use <code>column -t</code> to visualize data in the terminal, not to reformat data to write to a file. Tab-delimited data is preferable to data delimited by a variable number of spaces, since it\u2019s easier for programs to parse. Like <code>cut</code> , <code>column</code>\u2019s default delimiter is the tab character (<code>\\t</code> ). We can specify a different delimiter with the <code>-s</code> option. So, if we wanted to visualize the columns of the Mus_musculus.GRCm38.75_chr1_bed.csv file more easily, we could use:</p> <p>code</p> <pre><code>column -s \",\" -t Mus_musculus.GRCm38.75_chr1_bed.csv | head -n 3\n</code></pre> Output <pre><code>    1  3054233 3054733\n    1  3054233 3054733\n    1  3054233 3054733\n</code></pre> <ul> <li><code>column</code>: This command is used to format its input into multiple columns.</li> <li><code>-s \",\"</code>: This option specifies the separator character. In this case, it's set to a comma, which is typical for CSV files.</li> <li><code>-t</code>: This option determines the number of columns automatically and creates a table.</li> </ul> Counting the number of columns of a .gtf with <code>grep</code> and <code>wc</code> <p>GTF Format has 9 columns </p> 1 2 3 4 5 6 7 8 9 seqname source feature start end score strand frame attribute <p>In theory, we should be able to use the following command to isolate the column headers and count the number</p> <p><pre><code>grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | head -1 | wc -w\n</code></pre> However, this will return 16 and the problem is with the attribute column which is a \"A semicolon-separated list of tag-value pairs, providing additional information about each feature.\"</p> <p><pre><code>gene_id \"ENSMUSG00000090025\"; gene_name \"Gm16088\"; gene_source \"havana\"; gene_biotype \"pseudogene\";\n</code></pre> Therefore, we need to translate* these values to a single \"new line\" with <code>tr</code> command and then count the number of lines than than the number of words</p> <pre><code>grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | head -1 | tr '\\t' '\\n' | wc -l\n</code></pre>"},{"location":"4_inspectmanipluate/#sorting-plain-text-data-with-sort","title":"Sorting Plain-Text Data with <code>sort</code>","text":"<p>Very often we need to work with sorted plain-text data in bioinformatics. The two most common reasons to sort data are as follows:</p> <ul> <li>Certain operations are much more efficient when performed on sorted data.</li> <li>Sorting data is a prerequisite to finding all unique lines.</li> </ul> <p><code>sort</code>  is designed to work with plain-text data with columns. </p> <p>code</p> <ul> <li> <p>Create a test .bed file with few rows and use <code>sort</code> command without any arguments.</p> <p>Ctrl+D to end the edits started with <code>cat &gt;</code></p> </li> </ul> <pre><code>cat &gt; test_sort.bed\n</code></pre> input for the test_sort.bed file <pre><code>chr1    26  39\nchr1    32  47\nchr3    11  28\nchr1    40  49\nchr3    16  27\nchr1    9   28\nchr2    35  54\nchr1    10  19\n</code></pre> <p>code</p> <pre><code>sort test_sort.bed \n</code></pre> Output <pre><code>chr1    10  19\nchr1    26  39\nchr1    32  47\nchr1    40  49\nchr1    9   28\nchr2    35  54\nchr3    11  28\nchr3    16  27\n</code></pre> <p><code>sort</code> without any arguments simply sorts a file alphanumerically by line. Because chromosome is the first column, sorting by line effectively groups chromosomes together, as these are \"ties\" in the sorted order.</p> <p>However, using <code>sort</code>\u2019s default of sorting alphanumerically by line and doesn\u2019t handle tabular data properly. There are two new features we need:</p> <ul> <li> <p>The ability to sort by particular columns</p> </li> <li> <p>The ability to tell sort that certain columns are numeric values (and not alpha\u2010numeric text)</p> </li> </ul> <p><code>sort</code> has a simple syntax to do this. Let\u2019s look at how we\u2019d sort example.bed by chromosome (first column), and start position (second column):</p> <p>code</p> <pre><code>sort -k1,1 -k2,2n test_sort.bed\n</code></pre> Output <pre><code>chr1    9   28\nchr1    10  19\nchr1    26  39\nchr1    32  47\nchr1    40  49\nchr2    35  54\nchr3    11  28\nchr3    16  27\n</code></pre> <p>Here, we specify the columns (and their order) we want to sort by as <code>-k</code> arguments. In technical terms, <code>-k</code> specifies the sorting keys and their order. Each <code>-k</code> argument takes a range of columns as <code>start,end</code>, so to sort by a single column we use <code>start,start</code>. In the preceding example, we first sorted by the first column (chromosome), as the first <code>-k</code> argument was <code>-k1,1</code> . Sorting by the first column alone leads to many ties in rows with the same chromosomes (e.g., \u201cchr1\u201d and \u201cchr3\u201d). Adding a second <code>-k</code> argument with a different column tells sort how to break these ties. In our example, <code>-k2,2n</code> tells sort to sort by the second column (start position), treating this column as numerical data (because there\u2019s an <code>n</code> in <code>-k2,2n</code>).</p> <p>The end result is that rows are grouped by chromosome and sorted by start position.</p> Exercise 4.3 <p>Mus_musculus.GRCm38.75_chr1_random.gtf file is Mus_musculus.GRCm38.75_chr1.gtf with permuted rows (and without a metadata header). Can you group rows by chromosome, and sort by position? If yes, append the output to a separate file.</p> solution <pre><code>sort -k1,1 -k4,4n Mus_musculus.GRCm38.75_chr1_random.gtf &gt; Mus_musculus.GRCm38.75_chr1_sorted.gtf\n</code></pre>"},{"location":"4_inspectmanipluate/#finding-unique-values-with-uniq","title":"Finding Unique Values with <code>uniq</code>","text":"<p><code>uniq</code> takes lines from a file or standard input stream and outputs all lines with consecutive duplicates removed. While this is a relatively simple functionality, you will use <code>uniq</code> very frequently in command-line data processing.</p> <p>code</p> <pre><code>cat letters.txt \n</code></pre> Output <pre><code>A \nA\nB\nC\nB\nC\nC\nC\n</code></pre> <p>code</p> <pre><code>uniq letters.txt \n</code></pre> <pre><code>A\nB\nC\nB\nC\n</code></pre> <p>As you can see, <code>uniq</code> does not return the unique values in letters.txt \u2014 it only removes consecutive duplicate lines (keeping one). If instead we did want to find all unique lines in a file, we would first sort all lines using <code>sort</code> so that all identical lines are grouped next to each other, and then run <code>uniq</code>.</p> <p>code</p> <pre><code>sort letters.txt | uniq\n</code></pre> <pre><code>A\nB\nC\n</code></pre> <p><code>uniq</code> with <code>-c</code> shows the counts of occurrences next to the unique lines.</p> <p>code</p> <pre><code>sort letters.txt | uniq -c\n</code></pre> <pre><code>      2 A\n      2 B\n      4 C\n</code></pre> <p>Combined with other Unix tools like <code>cut</code>, <code>grep</code> and <code>sort</code>, <code>uniq</code> can be used to summarize columns of tabular data:</p> <p>code</p> <pre><code>grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | cut -f3 | sort | uniq -c\n</code></pre> Output <pre><code>25901 CDS\n36128 exon\n2027 gene\n2290 start_codon\n2299 stop_codon\n4993 transcript\n7588 UTR\n</code></pre> <p>Count in order from most frequent to last</p> <p>code</p> <pre><code>grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | cut -f3 | sort | uniq -c | sort -rn\n</code></pre> <pre><code>  36128 exon\n  25901 CDS\n   7588 UTR\n   4993 transcript\n   2299 stop_codon\n   2290 start_codon\n   2027 gene\n</code></pre> <ul> <li><code>n</code> and <code>r</code> represents numerical sort and reverse order (Or descending as the default as ascending) </li> </ul>"},{"location":"4_inspectmanipluate/#joining-two-files-with-join","title":"Joining two files with <code>join</code>","text":"<p>Prepare</p> <p>Create a file named example_lengths.txt with following content</p> <pre><code>chr1    58352\nchr2    39521\nchr3    24859\n</code></pre> <p>The Unix tool <code>join</code> is used to join different files together by a common column. For example, we may want to add chromosome lengths recorded in example_lengths.txt to example.bed BED file, we saw earlier. The files look like this:</p> <p>To do this, we need to join both of these tabular files by their common column, the one containing the chromosome names. But first, we first need to sort both files by the column to be joined on (join would not work otherwise):</p> <p>code</p> <pre><code>sort -k1,1 example.bed &gt; example_sorted.bed\n</code></pre> <ul> <li><code>-k1,1</code> : This option specifies the sorting key. Here's what it means:<ul> <li><code>-k</code>: This flag indicates that you are specifying a sorting key.</li> <li><code>1,1</code> : This means you are sorting based on the first field (column) of each line in the file. The syntax <code>1,1</code> indicates that the sort should start at the first field and end at the first field, meaning only the first field is considered for sorting.</li> </ul> </li> </ul> <p>code</p> <pre><code>sort -c -k1,1 example_lengths.txt\n</code></pre> <ul> <li><code>-c</code>, <code>--check</code>, <code>--check=diagnose-first</code> = check for sorted input; do not sort<ul> <li>When you use this option, <code>sort</code> does not <code>sort</code> the file; instead, it checks whether the file is already sorted according to the specified key. If the file is sorted correctly, it will exit with a status code of <code>0</code>. If the file is not sorted, it will exit with a status code of <code>1</code> and may print an error message indicating where the sorting order is violated.</li> </ul> </li> </ul> <p>The basic syntax is <code>join -1 &lt;file_1_field&gt; -2 &lt;file_2_field&gt; &lt;file_1&gt; &lt;file_2&gt;</code>. So, with example.bed and example_lengths.txt this would be:</p> <p>code</p> <pre><code>join -1 1 -2 1 example_sorted.bed example_lengths.txt &gt; example_with_lengths.txt\n</code></pre> <ul> <li><code>-1 1</code> : This option specifies the join field for the first file (example_sorted.bed). The -1 flag indicates which field from the first file to use for joining, and 1 refers to the first field (column).</li> <li><code>-2 1</code> :This option specifies the join field for the second file (example_lengths.txt). Similar to -1, the -2 flag indicates which field from the second file to use for joining, and 1 again refers to the first field (column).</li> </ul> <p>Inspect the file <pre><code>cat example_with_lengths.txt\n</code></pre></p> <p>There are many types of joins. For now, it\u2019s important that we make sure join is working as we expect. Our expectation is that this join should not lead to fewer rows than in our example.bed file. We can verify this with <code>wc -l</code>:</p> <p>code</p> <pre><code>wc -l example_sorted.bed example_with_lengths.txt \n</code></pre> <pre><code>  8 example_sorted.bed\n  8 example_with_lengths.txt\n 16 total\n</code></pre> <p>However, look what happens if our second file, example_lengths.txt doesn\u2019t have the lengths for chr3:</p> <p>code</p> <p><pre><code>head -n 2 example_lengths.txt &gt; example_lengths_alt.txt #truncate file\n</code></pre> <pre><code>join -1 1 -2 1 example_sorted.bed example_lengths_alt.txt\n</code></pre></p> <pre><code>chr1 10 19 58352\nchr1 26 39 58352\nchr1 32 47 58352\nchr1 40 49 58352\nchr1 9 28 58352\nchr2 35 54 39521\n</code></pre> <p>code</p> <pre><code>join -1 1 -2 1 example_sorted.bed example_lengths_alt.txt | wc -l\n</code></pre> <pre><code>6\n</code></pre> <p>Because chr3 is absent from example_lengths_alt.txt, our join omits rows from example_sorted.bed that do not have an entry in the first column of example_lengths_alt.txt. If we don\u2019t want this behavior, we can use option -a to include unpairable lines\u2014ones that do not have an entry in either file:</p> <p>code</p> <pre><code>join -1 1 -2 1 -a 1 example_sorted.bed example_lengths_alt.txt \n</code></pre> Output <pre><code>chr1 10 19 58352\nchr1 26 39 58352\nchr1 32 47 58352\nchr1 40 49 58352\nchr1 9 28 58352\nchr2 35 54 39521\nchr3 11 28\nchr3 16 27\n</code></pre> <ul> <li><code>-1 1</code>  : This option specifies that the join should use the first field of the first file (example_sorted.bed) as the key for joining. The <code>1</code> indicates that the key is in the first column of this file.</li> <li><code>-2 1</code>  : This option specifies that the join should use the first field of the second file (example_lengths_alt.txt) as the key for joining. Similarly, the <code>1</code> indicates that the key is in the first column of this file.</li> <li><code>-a 1</code> -  join command to include all lines from the first file (example_sorted.bed), even if there is no matching line in the second file. This is useful when you want to retain all records from the first file regardless of whether they find a match in the second file.</li> </ul> <p>Back to homepage</p>"},{"location":"5_inspectmanipulate2/","title":"4. Inspecting and Manipulating Text Data with Unix Tools - Part 2","text":"<p>Lesson Objectives</p> <ul> <li>insertion, deletion, search and replace(substitution) with <code>sed</code></li> <li>Use specialised language <code>awk</code> to do a variety of text-processing tasks</li> <li>Quick overview of <code>bioawk</code> (an extension of <code>awk</code> to process common biological data formats)</li> </ul> <p></p>"},{"location":"5_inspectmanipulate2/#sed","title":"sed","text":"<p>The <code>s</code>treamline <code>ed</code>itor or <code>sed</code> command is a stream editor that reads one or more text files, makes changes or edits according to editing script, and writes the results to standard output. First, we will discuss sed command with respect to search and replace function. </p>"},{"location":"5_inspectmanipulate2/#find-and-replace","title":"Find and Replace","text":"<p>Most common use of <code>sed</code> is to substitute text, matching a pattern. The syntax for doing this in <code>sed</code> is as follows:</p> <pre><code>sed 'OPERATION/REGEXP/REPLACEMENT/FLAGS' FILENAME\n</code></pre> <ul> <li>Here, <code>/</code> is the delimiter (you can also use <code>_</code> (underscore), <code>|</code> (pipe) or <code>:</code> (colon) as delimiter as well)</li> <li><code>OPERATION</code> specifies the action to be performed (sometimes if a condition is satisfied). <ul> <li>The most common and widely used operation is <code>s</code> which does the substitution operation </li> <li>Other useful operators include <code>y</code> for transformation, <code>i</code> for insertion, <code>d</code> for deletion etc.).</li> </ul> </li> <li><code>REGEXP</code> and <code>REPLACEMENT</code> specify search term and the substitution term respectively for the operation that is being performed.</li> <li><code>FLAGS</code> are additional parameters that control the operation. Some common <code>FLAGS</code> include:<ul> <li><code>g</code>   replace all the instances of <code>REGEXP</code> with <code>REPLACEMENT</code> (globally)</li> <li><code>N</code> where N is any number, to replace Nth instance of the <code>REGEXP</code> with <code>REPLACEMENT</code></li> <li><code>p</code> if substitution was made, then prints the new pattern space</li> <li><code>i</code> ignores case for matching <code>REGEXP</code></li> <li><code>w</code> file If substitution was made, write out the result to the given file</li> <li><code>d</code> when specified without <code>REPLACEMENT</code>, deletes the found <code>REGEXP</code></li> </ul> </li> </ul> <ul> <li>Some find and replace examples</li> </ul> <p>Find and replace all <code>chr</code> to <code>chromosome</code> in the example.bed file and append the the edit to a new file names example_chromosome.bed</p> <pre><code>sed 's/chr/chromosome/g' example.bed &gt; example_chromosome.bed\n</code></pre> <p>Find and replace <code>chr</code> to <code>chromosome</code>, only if you also find 40 in the line</p> <ul> <li>This will follow the format <code>sed '/SEARCH_STRING/OPERATION/REGEXP/REPLACEMENT/FLAGS' FILENAME</code> where <code>SEARCH_STRING</code> is 40 <pre><code>sed '/40/s/chr/chromosome/g' example.bed &gt; example_40.bed\n</code></pre></li> <li><code>/40/</code>: This is an address (a pattern match) that restricts the following command to only those lines containing the string \"40\"</li> </ul> Why is this useful ? <p>This command is particularly useful for processing genomic data files, where you might want to change the chromosome notation (from \"chr\" to \"chromosome\") only for a specific chromosome </p> <p>Find and replace directly on the input, but save an old version too</p> <pre><code>sed -i.old 's/chr/chromosome/g' example.bed\n</code></pre> <ul> <li><code>-i</code> to edit files in-place instead of printing to standard output</li> <li>original file will be retained under the filename <code>example.bed.old</code></li> </ul> Why is this useful ? <ul> <li>Standardizing nomenclature: In genomics, chromosomes are sometimes abbreviated as \"chr\" (e.g., chr1, chr2), but some tools or databases might require the full word \"chromosome\". This command helps standardize the format.</li> <li>File format conversion: It can help convert between different file formats or standards that use different chromosome naming conventions.</li> <li>Data cleaning: If you have a large dataset with inconsistent chromosome naming, this command can quickly standardize it.</li> <li>Preparing data for specific tools: Some bioinformatics tools might require a specific format for chromosome names, and this command can help prepare your data.</li> <li>Safety: The -i.old option creates a backup, allowing you to revert changes if needed.</li> <li>Efficiency: It can process large files quickly, which is common in genomics data.</li> <li>Reproducibility: This command can be easily incorporated into scripts or pipelines, ensuring consistent data processing.</li> </ul> <ul> <li>Print specific lines of the file</li> </ul> <p>To print a specific line you can use the address function. Note that by default, <code>sed</code> will stream the entire file, so when you are interested in specific lines only, you will have to suppress this feature using the option <code>-n</code> </p> <p><code>-n</code>, <code>--quiet</code>, <code>--silent</code> = suppress automatic printing of pattern space</p> <p>print 5<sup>th</sup> line of example.bed</p> <pre><code>sed -n '5p' example.bed\n</code></pre> <ul> <li><code>n:</code> This option suppresses the default output. Normally, sed prints every line of the input file to the standard output. The -n option tells sed to not print anything unless explicitly instructed to do so.</li> <li><code>'5p'</code>: This is the command within sed. The 5 specifies the line number, and p stands for print. So, 5p means \"print the 5<sup>th</sup> line\".</li> </ul> <p>We can provide any number of additional lines to print using <code>-e</code> option. This tells sed to execute the next command line argument as sed program. Let's print line 2 and 5,</p> <pre><code>sed -n -e '2p' -e '5p' example.bed\n</code></pre> <ul> <li><code>-e</code> This allows you to specify a script</li> <li><code>-e '2p'</code>: This is the first expression. The 2p part means \"print the 2<sup>nd</sup> line\".</li> <li><code>-e '5p'</code>: This is the second expression. The 5p part means \"print the 5<sup>th</sup> line\".</li> </ul> Why is this useful ? <ul> <li>Quality control: Checking if certain lines (e.g., the 2<sup>nd</sup> and 5<sup>th</sup>) are formatted correctly or contain expected data.</li> <li>Debugging scripts: Verifying that data processing steps are working as intended by looking at specific entries.</li> <li>Sampling: Quickly viewing a few lines without loading the entire file into memory.</li> </ul> <p>It also accepts range, using <code>,</code>. Let's print line 2-6,</p> <pre><code>sed -n '2,6p' example.bed\n</code></pre> <ul> <li><code>'2,6p'</code>: This is the command within sed. The 2,6 specifies the range of lines, and p stands for print. So, 2,6p means \"print lines 2 through 6\"</li> </ul> <p>Also, we can create specific pattern, like multiples of a number using <code>~</code>. Let's print every tenth line of Mus_musculus.GRCm38.75_chr1.bed starting from 10, 20, 30.. to end of the file</p> <pre><code>sed -n '10~10p' Mus_musculus.GRCm38.75_chr1.bed\n</code></pre> <ul> <li><code>'10~10p'</code>: This is the key part of the command. It uses a special addressing syntax:<ul> <li><code>10</code>: Start at line 10</li> <li><code>~10</code>: Then print every 10<sup>th</sup> line after that</li> <li><code>p</code>: The print command</li> </ul> </li> </ul> Why is this useful ? <ul> <li>Sampling Large Datasets: When files are too large to inspect in full, sampling every 10<sup>th</sup> line provides a manageable subset for quick quality checks or visualization.</li> <li>Performance Testing: Developers can use sampled data to test pipelines or software tools without processing the entire dataset.</li> <li>Data Summarization: Researchers may want to get a sense of the data distribution or content without loading the whole file into memory.</li> </ul> Exercise 4.4 <p>Can you use the above <code>~</code> trick to extract all the odd numbered lines from Mus_musculus.GRCm38.75_chr1.bed and append the output to a new file odd_sed.bed</p> <p>One of the powerful features is that we can combine these ranges or multiples in any fashion. Example: fastq files have header on first line and sequence in second, next two lines will have the quality and a blank extra line (four lines make one read). Sometimes  we only need the sequence and header</p> <p>code</p> <pre><code>sed -n '1~4p;2~4p' SRR097977.fastq\n</code></pre> <ul> <li><code>'1~4p;2~4p'</code>: This is the sed script, which consists of two address/action pairs separated by a semicolon:<ul> <li><code>1~4p</code>: This means \"starting at line 1, print every 4<sup>th</sup> line\"</li> <li><code>2~4p</code>: This means \"starting at line 2, print every 4<sup>th</sup> line\"</li> </ul> </li> </ul> Why is this userful ? <ul> <li>Quickly view or extract just the sequence data and identifiers from a FASTQ file</li> <li>Reduce the file size by removing quality score information</li> <li>Prepare the data for further processing that only requires the sequence and its identifier</li> </ul> <p>Sanity Check</p> <p>It's not a bad practice validate some of these commands by comparing the output from another command. For an example, above <code>sed -n '1~4p;2~4p' SRR097977.fastq</code> should print exactly half the number of lines in the file as it is removing two lines per read. Do a quick sanity check with <code>sed -n '1~4p;2~4p' SRR097977.fastq  | wc -l</code> &amp; <code>cat SRR097977.fastq | wc -l</code></p> <p>We can use the above trick to convert the .fastq to .fasta</p> <pre><code>sed -n '1~4p;2~4p' SRR097977.fastq  | sed 's/^@/&gt;/g' &gt; SRR097977.fasta\n</code></pre> <ul> <li><code>^@</code>: Matches the <code>'@'</code> character at the beginning of a line</li> <li><code>&gt;:</code> Replaces the matched <code>'@'</code> with <code>'&gt;'</code></li> </ul> Why is this useful <ul> <li>Format Conversion: It quickly converts a FASTQ file to a FASTA file, which is a common requirement in many bioinformatics workflows.</li> <li>Data Reduction: It removes the quality score information, which is not used in FASTA format, potentially reducing file size.</li> <li>Compatibility: Many bioinformatics tools work with FASTA format, so this conversion can be a necessary preprocessing step.</li> <li>Efficiency: It performs the conversion using efficient text-processing tools, which can be faster than some scripted solutions, especially for large files.</li> <li>Pipeline Integration: This one-liner can be easily integrated into larger bioinformatics pipelines or scripts.</li> </ul> <p></p> Optional (Advanced) -Let's say that we want capture all the transcript names from the last column (9<sup>th</sup> column) from .gtf file. We can write something similar to: <p>code</p> <pre><code>grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | head -n 3 | sed -E 's/.*transcript_id \"([^\"]+)\".*/\\1/'\n</code></pre> <p><code>-E</code> option to enable POSIX Extended Regular Expressions (ERE)</p> POSIX Regular and Exetended Regular Expressions <p>POSIX Basic Regular Expressions </p> <ul> <li> <p>POSIX or \u201cPortable Operating System Interface for uniX\u201d is a collection of standards that define some of the functionality that a (UNIX) operating system should support. One of these standards defines two flavors of regular expressions. Commands involving regular expressions, such as <code>grep</code> and <code>egrep</code>, implement these flavors on POSIX-compliant UNIX systems. Several database systems also use POSIX regular expressions.</p> <p>The Basic Regular Expressions or BRE flavor standardizes a flavor similar to the one used by the traditional UNIX <code>grep</code> command. This is pretty much the oldest regular expression flavor still in use today. One thing that sets this flavor apart is that most meta-characters require a backslash to give the metacharacter its flavor. Most other flavors, including POSIX ERE, use a backslash to suppress the meaning of metacharacters. Using a backslash to escape a character that is never a metacharacter is an error.</p> </li> </ul> <p>POSIX Extended Regular Expressions</p> <ul> <li> <p>The Extended Regular Expressions or ERE flavor standardizes a flavor similar to the one used by the UNIX <code>egrep</code> command. \u201cExtended\u201d is relative to the original UNIX <code>grep</code>, which only had bracket expressions, dot, caret, dollar and star. An ERE support these just like a BRE. Most modern regex flavors are extensions of the ERE flavor. By today\u2019s standard, the POSIX ERE flavor is rather bare bones. The POSIX standard was defined in 1986, and regular expressions have come a long way since then.</p> <p>The developers of <code>egrep</code> did not try to maintain compatibility with <code>grep</code>, creating a separate tool instead. Thus <code>egrep</code>, and POSIX ERE, add additional metacharacters without backslashes. You can use backslashes to suppress the meaning of all metacharacters, just like in modern regex flavors. Escaping a character that is not a meta-character is an error.</p> </li> </ul> <p>Output is not really what we are after,</p> <pre><code>1  pseudogene  gene    3054233 3054733 .   +   .   gene_id \"ENSMUSG00000090025\"; gene_name \"Gm16088\"; gene_source \"havana\"; gene_biotype \"pseudogene\";\nENSMUST00000160944\nENSMUST00000160944\n</code></pre> <p>The is due to <code>sed</code> default behaviour where it prints every line, making replacements to matching lines. .i.e Some lines of the last column of Mus_musculus.GRCm38.75_chr1.gtf don't contain <code>transcript_id</code>. So, <code>sed</code> prints the entire line rather than captured group. One way to solve this would be to use <code>grep</code> <code>transcript_id</code> before <code>sed</code> to only work with lines containing the string <code>transcript_id</code> . However, <code>sed</code> offers a cleaner way. First, disable <code>sed</code> from outputting all lines with <code>-n</code> ( can use <code>--quiet</code> or <code>--silent</code> as well .i.e. suppress automatic printing of pattern space). Then, by appending <code>p</code> (Print the current pattern space) after the last slash <code>sed</code> will print all lines it\u2019s made a replacement on. The following is an illustration of <code>-n</code> used with <code>p</code>:</p> <p>code</p> <pre><code>grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | head -n 3 | sed -E -n 's/.*transcript_id \"([^\"]+)\".*/\\1/p'\n</code></pre> <p>This example uses an important regular expression idiom: capturing text between delimiters (in this case, quotation marks). This is a useful pattern, so let\u2019s break it down:</p> <ol> <li>First, match zero or more of any character ( .* ) before the string \"transcript_id\" .</li> <li>Then, match and capture (because there are parentheses around the pattern) one or more characters that are not a quote. This is accomplished with [^\"]+ , the important idiom in this example. In regular extension jargon, the brackets make up a character class. Character classes specify what characters the expression is allowed to match. Here, we use a caret ( ^ ) inside the brackets to match anything except what\u2019s inside these brackets (in this case, a quote). The end result is that we match and capture one or more nonquote characters (because there\u2019s a trailing + ). This approach is nongreedy; often beginners make the mistake of taking a greedy approach and use .* . Consider:</li> </ol>"},{"location":"5_inspectmanipulate2/#aho-weinberger-kernighan-awk","title":"Aho, Weinberger, Kernighan = AWK","text":"<p>Awk is a scripting language used for manipulating data and generating reports. The awk command programming language requires no compiling and allows the user to use variables, numeric functions, string functions, and logical operators. </p> <p>Awk is a utility that enables a programmer to write tiny but effective programs. These take the form of statements that define text patterns that are to be searched for in each line of a document, and the action that is to be taken when a match is found within a line. </p> <p>Awk is mostly used for pattern scanning and processing. It searches one or more files to see if they contain lines that match with the specified patterns and then perform the associated actions. </p> WHAT CAN WE DO WITH AWK? <ol> <li> <p>AWK Operations: </p> <ul> <li>Scans a file line by line </li> <li>Splits each input line into fields </li> <li>Compares input line/fields to pattern </li> <li>Performs action(s) on matched lines </li> </ul> </li> <li> <p>Useful For: </p> <ul> <li>Transform data files </li> <li>Automation</li> <li>Produce formatted reports </li> </ul> </li> <li> <p>Programming Constructs: </p> <ul> <li>Format output lines </li> <li>Arithmetic and string operations </li> <li>Conditionals and loops </li> </ul> </li> </ol> <p>There are two key parts for understanding the Awk language: how Awk processes records, and pattern-action pairs. The rest of the language is quite simple.</p> <ul> <li> <p>Awk processes input data a record (line) at a time. Each record is composed of fields (column entries) that Awk automatically separates. Awk assigns the entire record to the variable $0, field one\u2019s value to $1, field two\u2019s value to $2, etc.</p> </li> <li> <p>We build Awk programs using one or more of the following structures: <code>pattern { action }</code>. Each pattern is an expression or regular expression pattern. In Awk lingo, these are pattern-action pairs and we can chain multiple pattern-action pairs together (separated by semicolons). If we omit the pattern, Awk will run the action on all records. If we omit the action but specify a pattern, Awk will print all records that match the pattern.</p> </li> </ul> <p>Syntax:</p> <pre><code>awk options 'selection_criteria {action}' input-file &gt;  output-file\n</code></pre> <ul> <li> <p><code>'selection_criteria {action}'</code>: This is the core part of the awk command where you define:</p> <ul> <li><code>selection_criteria</code>: Conditions that determine which lines or records in the input file are processed. The selection criteria can be any valid <code>awk</code> expression that evaluates to true or false. Some common examples include:</li> <li><code>action</code>: Operations to perform on the selected lines or records. .i.e. The action is a block of code enclosed in <code>{}</code> that specifies what to do with the selected lines</li> </ul> </li> </ul> <p>How does it Work:</p> <ul> <li>AWK reads the input file line by line.</li> <li>For each line, it checks if the line matches the <code>selection_criteria</code>.</li> <li>If a line matches, AWK executes the action on that line.</li> <li>If no action is specified, AWK prints the matching line by default</li> </ul> <p>Default behaviour of <code>awk</code> is to print every line of data from the specified file. .i.e. mimics <code>cat</code></p> <pre><code>awk '{print}' example.bed \n</code></pre> Output <pre><code>chromosome1 26  39\nchromosome1 32  47\nchromosome3 11  28\nchromosome1 40  49\nchromosome3 16  27\nchromosome1 9   28\nchromosome2 35  54\nchromosome1 10  19\n</code></pre> <p>Print lines which match the given pattern</p> <pre><code>awk '/chromosome1/{print}' example.bed\n</code></pre> <pre><code>chromosome1    26  39\nchromosome1    32  47\nchromosome1    40  49\nchromosome1    9   28\nchromosome1    10  19\n</code></pre> <p><code>awk</code> can be used to mimic functionality of <code>cut</code> : Following command is useful for extracting specific columns from a tabular file, which is a common operation in data processing and bioinformatics workflows.</p> <pre><code>awk '{print $2 \"\\t\" $3}' example.bed \n</code></pre> <ul> <li><code>$2</code>: This refers to the second field (column) of the current line.</li> <li><code>\"\\t\"</code>: This adds a tab character between the output fields. ( Refer to Special meanings of certain escaped characters in supplementary )</li> <li><code>$3</code>: This refers to the third field (column) of the current line.</li> </ul> <p>Here, we\u2019re making use of Awk\u2019s string concatenation. Two strings are concatenated if they are placed next to each other with no argument. So for each record, <code>$2\"\\t\"$3</code> concatenates the second field, a tab character, and the third field. However, this is an instance where using <code>cut</code> is much simpler as the equivalent of above is <code>cut -f 2,3 example.bed</code> </p> Output <pre><code>26  39\n32  47\n11  28\n40  49\n16  27\n 9  28\n35  54\n10  19\n</code></pre> <p>Let\u2019s look at how we can incorporate simple pattern matching. Suppose we wanted to write a filter that only output lines where the length of the feature (end position - start position) was greater than 18. Awk supports arithmetic with the standard operators + , - , * , / , % (remainder), and ^ (exponentiation). We can subtract within a pattern to calculate the length of a feature, and filter on that expression:</p> <pre><code>awk '$3 - $2 &gt; 18' example.bed\n</code></pre> Output <pre><code>chromosome1    9   28\nchromosome2    35  54\n</code></pre> <ul> <li><code>$3</code>: This refers to the third field (column) of the current line.</li> <li><code>$2</code>: This refers to the second field (column) of the current line.</li> <li><code>$3 - $2</code>: This calculates the difference between the third and second fields.<ul> <li>Space on either side of <code>-</code> is not important but leaving a space makes it easy to read</li> </ul> </li> <li><code>&gt; 18</code>: This checks if the difference is greater than 18.</li> </ul> Why is this useful <ul> <li>Filtering Genomic Regions by Length:<ul> <li>In a BED file, the second and third columns typically represent the start and end positions of genomic regions. This command can filter out regions that are shorter than a specified length (in this case, 18 base pairs).</li> </ul> </li> <li>Quality Control:     When working with genomic data, you might want to exclude very short regions that could be artifacts or less reliable. This command helps in maintaining the quality of your dataset by keeping only those regions that meet a minimum length criterion.</li> <li>Data Reduction:     If you have a large dataset and are only interested in longer regions, this command can help reduce the size of the dataset, making subsequent analyses faster and more manageable.</li> <li>Custom Analysis:     In some analyses, you might be specifically interested in longer genomic regions due to their biological significance. This command allows you to focus on those regions directly.</li> </ul> <p>Example Scenario Imagine you are analyzing ChIP-Seq data to identify binding sites of a transcription factor. You might want to filter out very short binding sites to focus on more substantial binding events. The command can help you achieve this: <pre><code>awk '$3 - $2 &gt; 18' chipseq_peaks.bed &gt; filtered_peaks.bed\n</code></pre></p> <code>awk</code> Comparison and Logical operations <p>AWK provides a set of comparison (relational) operators to compare numbers or strings, and to match patterns. These are used to filter data or control the flow of actions in AWK scripts. The main comparison operators are</p> <ul> <li>Comparisons can be numeric or string-based, depending on the data type of the operands.</li> <li>The <code>~</code> and <code>!~</code> operators are used for regular expression matching.</li> </ul> Comparison Description <code>a == b</code> a is equal to b <code>a != b</code> a is not equal to b <code>a &lt; b</code> a is less than b <code>a &gt; b</code> a is greater than b <code>a &lt;= b</code> a is less than or equal to b <code>a &gt;= b</code> a is greater than or equal to b <code>a ~ b</code> a matches regular expression pattern b <code>a !~ b</code> a does not match regular expression pattern b <code>a &amp;&amp; b</code> logical a and b <code>a \\|\\| b</code> logical or a and b <code>!a</code> not a (logical negation) <p>We can also chain patterns, by using logical operators <code>&amp;&amp;</code> (AND), <code>||</code> (OR), and <code>!</code> (NOT). For example, if we wanted all lines on chromosome 1 with a length greater than 10:</p> <p>code</p> <pre><code>awk '$1 ~ /chromosome1/ &amp;&amp; $3 - $2 &gt; 10' example.bed \n</code></pre> Output <pre><code>chromosome1    26  39\nchromosome1    32  47\nchromosome1    9   28\n</code></pre> <ul> <li>First pattern, <code>$1 ~ /chr1</code> specifies the regular expression (All Regular expressions are in slashes).  We are matching the first field, <code>$1</code> against the regular expression <code>chr1</code>. </li> <li>Tilde <code>~</code> means match.</li> <li>To do the inverse of match, we can use <code>!~</code> OR <code>!($1 ~ /chromosome1/)</code></li> </ul> <p>Built-In Variables and special patterns In Awk</p> <p>Awk\u2019s built-in variables include the field variables <code>$1</code>, <code>$2</code>, <code>$3</code>, and so on (<code>$0</code> is the entire line) \u2014 that break a line of text into individual words or pieces called fields. </p> <ul> <li> <p>Control Variables - Some built-in variables are used to control how awk operates.</p> <ul> <li><code>FS</code> (field separator): contains the field separator character which is used to divide fields on the input line. The default is \u201cwhite space\u201d, meaning space and tab characters. FS can be reassigned to another character (typically in BEGIN) to change the field separator. </li> <li><code>OFS</code> ( Output field separator): stores the output field separator, which separates the fields when Awk prints them. The default is a blank space. Whenever print has several parameters separated with commas, it will print the value of OFS in between each parameter. </li> <li><code>RS</code> (Record Separator): stores the current record separator character. Since, by default, an input line is the input record, the default record separator character is a newline. </li> <li><code>ORS</code> (Output record separator ): stores the output record separator, which separates the output lines when Awk prints them. The default is a newline character. print automatically outputs the contents of ORS at the end of whatever it is given to print. </li> </ul> </li> <li> <p>Information Variables - : These variables provide information about the current state of the program:</p> <ul> <li><code>NR</code> (Number of records): keeps a current count of the number of input records. Remember that records are usually lines. Awk command performs the pattern/action statements once for each record in a file. </li> <li><code>NF</code> (Number of fields): keeps a count of the number of fields within the current input record. </li> <li><code>FNR</code> (File Number of Records): Keeps track of the number of records read from the current input file.</li> <li><code>FILENAME</code>: Contains the name of the current input file being processed.</li> </ul> </li> </ul> <p>Also, there are two special patterns <code>BEGIN</code> &amp; <code>END</code> - These special patterns enable more complex and flexible data processing workflows by allowing actions to be tied to specific conditions and stages of the input processing cycle</p> <ul> <li><code>BEGIN {action}</code> - specifies what to do before the first record is read in. Useful to initialise and set up variables</li> <li><code>END {action}</code> - what to do after the last record's processing is complete. Useful to print data summaries at the end of file processing</li> </ul> <p>Examples</p> <p>We can use <code>NR</code> to extract ranges of lines, too; for example, if we wanted to extract all lines between 3 and 5 (inclusive):</p> <pre><code>awk 'NR &gt;= 3 &amp;&amp; NR &lt;=5' example.bed\n</code></pre> Output <pre><code>    chr3   11  28\n    chr1   40  49\n    chr3   16  27\n</code></pre> <p><code>'NR &gt;= 3 &amp;&amp; NR &lt;= 5'</code>: This is the selection criteria used by awk to determine which lines to process. Let's dissect this part further:</p> <ul> <li><code>NR</code>: This is a built-in variable in awk that stands for \"Number of Records.\" It keeps track of the current line number in the input file.</li> <li><code>&gt;= 3</code>: This means \"greater than or equal to 3.\"</li> <li><code>&amp;&amp;</code>: This is the logical AND operator.</li> <li><code>&lt;= 5</code>: This means \"less than or equal to 5.</li> </ul> Why is this useful <ul> <li>Extract a specific subset of lines from a larger file</li> <li>Perform operations on a range of lines in a file</li> <li>Debug or inspect specific portions of a large dataset</li> <li>It's commonly used in bioinformatics (as the .bed file extension suggests) and other fields where processing specific sections of large datasets is necessary.</li> </ul> <p>suppose we wanted to calculate the mean feature length in example.bed. We would have to take the sum feature lengths, and then divide by the total number of records. We can do this with:</p> <pre><code>awk 'BEGIN{s = 0}; {s += ($3-$2)}; END{ print \"mean: \" s/NR};' example.bed \n\n  mean: 14\n</code></pre> Explain please <ul> <li><code>'BEGIN{s = 0};</code><ul> <li><code>BEGIN{}</code>: This block is executed before any input lines are read.</li> <li><code>s = 0</code>: Initializes a variable s to 0. This variable will be used to accumulate the sum of differences between the third and second columns.</li> </ul> </li> <li><code>{s += ($3-$2)};</code><ul> <li>{}: This block is executed for each input line.</li> <li><code>s += ($3-$2)</code>: For each line, it calculates the difference between the third field (<code>$3</code>) and the second field (<code>$2</code>) and adds this difference to the variable <code>s</code>.</li> </ul> </li> <li> <p><code>END{ print \"mean: \" s/NR};</code></p> <ul> <li><code>END{}</code>: This block is executed after all input lines have been read.</li> <li><code>print \"mean: \" s/NR</code>: Prints the mean of the differences calculated. s is the sum of all differences, and NR (Number of Records) is the total number of lines read. So, <code>s/NR</code> gives the mean difference.</li> </ul> </li> <li> <p>Purpose of <code>{}</code> (Curly Braces)</p> <ul> <li>Curly braces <code>{}</code> enclose the action part of an AWK pattern-action statement.</li> <li>The action inside <code>{}</code> is executed when the pattern (or special block like <code>BEGIN</code> or <code>END</code>) is matched or triggered</li> </ul> </li> <li> <p>Purpose of <code>;</code> (Semicolon)</p> <ul> <li>Semicolons <code>;</code> in AWK separate multiple pattern-action statements when written on a single line.</li> <li>They allow you to write several AWK rules in one command line, making the script concise.</li> </ul> </li> </ul> <p>Summary - In this example, we\u2019ve initialized a variable <code>s</code> to 0 in <code>BEGIN</code> (variables you define do not need a dollar sign). Then, for each record we increment <code>s</code> by the length of the feature. At the end of the records, we print this sum <code>s</code> divided by the number of records <code>NR</code> , giving the mean.</p> Why is this useful ? <ul> <li>Genomic Feature Analysis: It quickly provides the average length of genomic features (e.g., genes, exons, or other annotated regions) in a BED file.</li> <li>Quality Control: It can be used to check if the features in your BED file have expected lengths, helping to identify potential errors or anomalies.</li> <li>Data Characterization: This simple statistic can give you a quick overview of the type of features in your file (e.g., short features like SNPs vs. longer features like genes).</li> <li>Comparative Analysis: You can use this command on multiple BED files to compare average feature lengths across different datasets or genomic regions.</li> <li>Preprocessing Step: Knowing the average feature length can be useful for downstream analyses or for setting parameters in other bioinformatics tools.</li> <li>Efficiency: It processes the entire file in a single pass, making it efficient for large genomic datasets.</li> <li>Flexibility: The command can be easily modified to calculate other statistics or to handle different file formats with similar structures</li> </ul> <p><code>awk</code> makes it easy to convert between bioinformatics files like BED and GTF. Can you generate a three-column BED file from Mus_muscu\u2010lus.GRCm38.75_chr1.gtf: ?</p> <ul> <li>Follow this link for a quick recap on annotation formats</li> <li> <p>Note that the start site of features in the .bed file is 1 less than the start site of features in the .gtf file: .bed uses 0-indexing (first position is numbered 0.) and .gtf uses 1-indexing (first position is numbered 1) .i.e. \"chr 1 100\" in a GTF/GFF is \"chr 0 100\" in BED</p> </li> <li> <p>Let's build the command based on <code>awk options 'selection_criteria {action}' input-file</code></p> <ul> <li>What is the <code>selection_criteria</code> ? ( hint- it evolves around a symbol. )</li> <li>There is a possibility of needing the \"inverse of match\" function in <code>awk</code> here which can be invoked with <code>!</code></li> <li>For <code>{action}</code>, we will need the field IDs of the three columns. Based on pre-existing knowledge of annotation formats, it will be feild 1, 4 and 5. Don't foget the above note on 0 vs 1 indexing of start site </li> </ul> </li> </ul>  Help ! <pre><code>awk '!/^#/ { print $1 \"\\t\" $4-1 \"\\t\" $5}' Mus_musculus.GRCm38.75_chr1.gtf | head -n 3\n</code></pre> <pre><code>1  3054232 3054733\n1  3054232 3054733\n1  3054232 3054733\n</code></pre> Explain...... <ul> <li><code>'!/^#/ { print $1 \"\\t\" $4-1 \"\\t\" $5}'</code><ul> <li><code>!/^#/</code> : This is a pattern that matches lines that do not start with the # character. The <code>^</code> symbol represents the start of a line, and <code>#</code> is the character to match. The <code>!</code> negates the match, so this pattern matches lines that do not start with <code>#</code>.</li> </ul> </li> <li><code>{ print $1 \"\\t\" $4-1 \"\\t\" $5}</code> This action block is executed for each line that matches the pattern (i.e., lines that do not start with <code>#</code>).<ul> <li><code>print $1</code>: Prints the first field of the line.</li> <li><code>\"\\t\"</code>: Inserts a tab character.</li> <li><code>$4-1</code>: Prints the value of the fourth field minus 1.</li> <li><code>\"\\t\"</code>: Inserts another tab character.</li> <li><code>$5</code>: Prints the fifth field of the line.</li> </ul> </li> </ul> <p></p> Optional (Advanced)  - <code>awk</code> also has a very useful data structure known as an associative array. Associative arrays behave like Python\u2019s dictionaries or hashes in other languages. We can create an associative array by simply assigning a value to a key. <p>For example, suppose we wanted to count the number of features (third column) belonging to the gene \u201cLypla1.\u201d We could do this by incrementing their values in an associative array:</p> <pre><code>awk '/Lypla1/ {feature[$3] += 1}; END {for (k in feature) print k \"\\t\" feature[k]}' Mus_musculus.GRCm38.75_chr1.gtf \n</code></pre> <pre><code>exon   69\nCDS    56\nUTR    24\ngene   1\nstart_codon    5\nstop_codon 5\ntranscript 9\n</code></pre> Quick Intro to Arrays <p>The <code>awk</code> language provides one-dimensional arrays for storing groups of related strings or numbers. Every <code>awk</code> array must have a name. Array names have the same syntax as variable names; any valid variable name would also be a valid array name. But one name cannot be used in both ways (as an array and as a variable) in the same <code>awk</code> program.</p> <p>Arrays in <code>awk</code> superficially resemble arrays in other programming languages, but there are fundamental differences. In <code>awk</code>, it isn\u2019t necessary to specify the size of an array before starting to use it. Additionally, any number or string, not just consecutive integers, may be used as an array index.</p> <p>In most other languages, arrays must be declared before use, including a specification of how many elements or components they contain. In such languages, the declaration causes a contiguous block of memory to be allocated for that many elements. Usually, an index in the array must be a nonnegative integer. For example, the index zero specifies the first element in the array, which is actually stored at the beginning of the block of memory. Index one specifies the second element, which is stored in memory right after the first element, and so on. It is impossible to add more elements to the array, because it has room only for as many elements as given in the declaration. (Some languages allow arbitrary starting and ending indices\u2014e.g., \u201815 .. 27\u2019\u2014but the size of the array is still fixed when the array is declared.)</p> <p>A contiguous array of four elements might look like below, conceptually, if the element values are eight, \"foo\", \"\", and 30.</p> <p></p> <p>Only the values are stored; the indices are implicit from the order of the values. Here, eight is the value at index zero, because eight appears in the position with zero elements before it.</p> <p>Arrays in <code>awk</code> are different\u2014they are associative. This means that each array is a collection of pairs\u2014an index and its corresponding array element value:</p> <p></p> <p>The pairs are shown in jumbled order because their order is irrelevant</p> <p>One advantage of associative arrays is that new pairs can be added at any time. For example, suppose a tenth element is added to the array whose value is \"number ten\". The result is:</p> <p></p> <p>Now the array is sparse, which just means some indices are missing. It has elements 0\u20133 and 10, but doesn\u2019t have elements 4, 5, 6, 7, 8, or 9.</p> <p>It\u2019s worth noting that there\u2019s an entirely Unix way to count features of a particular gene: <code>grep</code> , <code>cut</code> , <code>sort</code> , and <code>uniq -c</code></p> <p>code</p> <pre><code>grep \"Lypla1\" Mus_musculus.GRCm38.75_chr1.gtf | cut -f 3 | sort | uniq -c\n</code></pre> <p>However, if we needed to also filter on column-specific information (e.g., strand), an approach using just base Unix tools would be quite messy. With Awk, adding an additional filter would be trivial: we\u2019d just use <code>&amp;&amp;</code> to add another expression in the pattern.</p>"},{"location":"5_inspectmanipulate2/#optional-bioawk","title":"Optional - <code>bioawk</code>","text":"<p><code>bioawk</code> is an extension of <code>awk</code>, adding the support of several common biological data formats, including optionally gzip'ed BED, GFF, SAM, VCF, FASTA/Q and TAB-delimited formats with column names. It also adds a few built-in functions and a command line option to use TAB as the input/output delimiter. When the new functionality is not used, <code>bioawk</code> is intended to behave exactly the same as the original <code>awk</code>.</p> <p>The original <code>awk</code> requires a YACC-compatible parser generator (e.g. Byacc or Bison). <code>bioawk</code> further depends on zlib so as to work with gzip'd files.</p> YACC <p>A parser generator is a program that takes as input a specification of a syntax, and produces as output a procedure for recognizing that language. Historically, they are also called compiler-compilers.   YACC (yet another compiler-compiler) is an LALR(LookAhead, Left-to-right, Rightmost derivation producer with 1 lookahead token) parser generator. YACC was originally designed for being complemented by Lex.</p> <ul> <li>Lex (A Lexical Analyzer Generator) helps write programs whose control flow is directed by instances of regular expressions in the input stream. It is well suited for editor-script type transformations and for segmenting input in preparation for a parsing routine. </li> </ul> <p><code>bioawk</code> features</p> <ul> <li>It can automatically recognize some popular formats and will parse different features associated with those formats. The format option is passed to <code>bioawk</code> using <code>-c arg</code> flag. Here <code>arg</code> can be bed, sam, vcf, gff or fastx (for both fastq and FASTA). It can also deal with other types of table formats using the <code>-c header</code> option. When <code>header</code> is specified, the field names will used for variable names, thus greatly expanding the utility.`</li> <li>There are several built-in functions (other than the standard <code>awk</code> built-ins), that are specific to biological file formats. When a format is read with <code>bioawk</code>, the fields get automatically parsed. You can apply several functions on these variables to get the desired output. Let\u2019s say, we read fasta format, now we have <code>$name</code> and <code>$seq</code> that holds sequence name and sequence respectively. You can use the <code>print</code> function (<code>awk</code> built-in) to print <code>$name</code> and <code>$seq</code>. You can also use <code>bioawk</code> built-in with the <code>print</code> function to get length, reverse complement etc by using <code>'{print length($seq)}'</code>. Other functions include <code>reverse</code>, <code>revcomp</code>, <code>trimq</code>, <code>and</code>, <code>or</code>, <code>xor</code> etc.</li> </ul> Variables for each format <p>For the <code>-c</code> you can either specify bed, sam, vcf, gff, fastx or header. <code>bioawk</code> will parse these variables for the respective format. If <code>-c</code> header is specified, the field names (first line) will be used as variables (spaces and special characters will be changed to under_score).</p> bed sam vcf gff fastx chrom qname chrom seqname name start flag pos source seq end rname id feature qual name pos ref start comment score mapq alt end strand cigar qual score thickstart rnext filter filter thickend pnext info strand rgb tlen group blockcount seq attribute blocksizes qual blockstarts <p>bioawk is not a default linux/unix utility. .i.e. Has to be installed. This is available as a module on NeSI HPC platforms which can be loaded with </p> <p>code</p> <pre><code>module load bioawk/1.0\n</code></pre> <p>The basic idea of Bioawk is that we specify what bioinformatics format we\u2019re working with, and Bioawk will automatically set variables for each field (just as regular Awk sets the columns of a tabular text file to $1, $1, $2, etc.). For Bioawk to set these fields, specify the format of the input file or stream with -c. Let\u2019s look at Bioawk\u2019s supported input formats and what variables these formats set:</p> <p>code</p> <pre><code>bioawk\n</code></pre> <pre><code>usage: bioawk [-F fs] [-v var=value] [-c fmt] [-tH] [-f progfile | 'prog'] [file ...]\n</code></pre> <pre><code>bioawk -c help\n</code></pre> <pre><code>bed:\n   1:chrom 2:start 3:end 4:name 5:score 6:strand 7:thickstart 8:thickend 9:rgb 10:blockcount 11:blocksizes 12:blockstarts \nsam:\n   1:qname 2:flag 3:rname 4:pos 5:mapq 6:cigar 7:rnext 8:pnext 9:tlen 10:seq 11:qual \nvcf:\n   1:chrom 2:pos 3:id 4:ref 5:alt 6:qual 7:filter 8:info \ngff:\n   1:seqname 2:source 3:feature 4:start 5:end 6:score 7:filter 8:strand 9:group 10:attribute \nfastx:\n   1:name 2:seq 3:qual 4:comment \n</code></pre> <p>As an example of how this works, let\u2019s read in example.bed and append a column with the length of the feature (end position - start position) for all protein coding genes:</p> <p>code</p> <pre><code>bioawk -c gff '$3 ~ /gene/ &amp;&amp; $2 ~ /protein_coding/ {print $seqname,$end-$start}' Mus_musculus.GRCm38.75_chr1.gtf | head -n 4\n</code></pre> <pre><code>1  465597\n1  16807\n1  5485\n1  12533\n</code></pre> <p>Bioawk is also quite useful for processing FASTA/FASTQ files. For example, we could use it to turn a FASTQ file into a FASTA file:</p> <p>code</p> <pre><code>bioawk -c fastx '{print \"&gt;\"$name\"\\n\"$seq}' SRR097977.fastq | head -n 4\n</code></pre> <pre><code>&gt;SRR097977.1\nTATTCTGCCATAATGAAATTCGCCACTTGTTAGTGT\n&gt;SRR097977.2\nGGTTACTCTTTTAACCTTGATGTTTCGACGCTGTAT\n</code></pre> <p>Note that Bioawk detects whether to parse input as FASTQ or FASTA when we use <code>-c fastx</code>.</p> <p>Bioawk can also serve as a method of counting the number of FASTQ/FASTA entries:</p> <pre><code>bioawk -c fastx 'END{print NR}' SRR097977.fastq \n</code></pre> <p>Bioawk\u2019s function <code>revcomp()</code> can be used to reverse complement a sequence:</p> <pre><code>bioawk -c fastx '{print \"&gt;\"$name\"\\n\"revcomp($seq)}' SRR097977.fastq | head -n 4\n</code></pre> <pre><code>&gt;SRR097977.1\nACACTAACAAGTGGCGAATTTCATTATGGCAGAATA\n&gt;SRR097977.2\nATACAGCGTCGAAACATCAAGGTTAAAAGAGTAACC\n</code></pre> <p>Back to homepage</p>"},{"location":"6_automate_fileprocessing_find_xargs/","title":"5. Preview : Automating File-Processing with <code>find</code> and <code>xargs</code>","text":"This section will be moved to \"Advanced Shell for BioInformaitcs\" <p>In this section, we\u2019ll learn about a more powerful way to specify files matching some criteria using Unix <code>find</code>. We\u2019ll also see how files printed by <code>find</code> can be passed to another tool called <code>xargs</code> to create powerful Unix-based processing workflows.</p> <p>Suppose you have a program named <code>analyse_fastq</code> that takes multiple filenames through a standard process. If you wanted to run this program on all files with the suffix .fastq, you might run:</p> <pre><code>ls *.fastq | analyse_fastq\n</code></pre> <p>Fail</p> <p>Your shell expands this wildcard to all matching files in the current directory, and <code>ls</code> prints these filenames. Unfortunately, this leads to a common complication that makes <code>ls</code> and wildcards a fragile solution. Suppose your directory contains a filename called treatment 03.fastq. In this case, <code>ls</code> returns treatment 03.fastq along with other files. However, because files are separated by spaces, and this file contains a space, <code>analyse_fastq</code> will interpret treatment 03.fastq as two separate files, named treatment and 03.fastq. This problem crops up periodically in different ways, and it\u2019s necessary to be aware of when writing file-processing pipelines. Note that this does not occur with file \"globbing\" in arguments\u2014if <code>analyse_fastq</code> takes multiple files as arguments, your shell handles this properly:</p> <p>Note that this does not occur with file globbing in arguments\u2014if <code>analyse_fastq</code> takes multiple files as arguments, your shell handles this properly: <pre><code>analyse_fastq *.fastq\n</code></pre> Here, shell automatically escapes the space in the filename treatment 03.fastq, so <code>analyse_fastq</code> will correctly receive the arguments treatment-02.fastq, treatment-03.fastq,. The potential problem here is that there\u2019s a limit to the number of files that can be specified as arguments. The limit is high, but you can reach it with NGS data. In this case you may get a meassage: <code>: cannot execute [Argument list too long]</code> </p> Globbing <p>Bash does not support native regular expressions like some other standard programming languages. The Bash shell feature that is used for matching or expanding specific types of patterns is called globbing. Globbing is mainly used to match filenames or searching for content in a file. Globbing uses wildcard characters to create the pattern. The most common wildcard characters that are used for creating globbing patterns are described below.</p> <ol> <li> <p>Question mark \u2013 (<code>?</code>) </p> <ul> <li><code>?</code> is used to match any single character. You can use \u2018?\u2019 for multiple times for matching multiple characters.</li> </ul> </li> <li> <p>Asterisk \u2013 (<code>*</code>)</p> <ul> <li><code>*</code> is used to match zero or more characters. If you have less information to search any file or information then you can use \u2018*\u2019 in globbing pattern.</li> </ul> </li> <li> <p>Square Bracket \u2013 (<code>[]</code>)</p> <ul> <li><code>[]</code> is used to match the character from the range. Some of the mostly used range declarations are mentioned below.</li> </ul> </li> <li> <p>Caret \u2013 (<code>^</code>)</p> <ul> <li>You can use <code>^</code> with square bracket to define globbing pattern more specifically. <code>^</code> can be used inside or outside of square bracket. <code>^</code> is used outside the square bracket to search those contents of the file that starts with a given range of characters. <code>^</code>is used inside the square bracket to show all content of the file by highlighting the lines start with a given range of characters . </li> </ul> </li> </ol> <p>Solution</p> <p>Solution to both of the above problems is through <code>find</code> and <code>xargs</code>, as we will see in the following sections.</p>"},{"location":"6_automate_fileprocessing_find_xargs/#finding-files-with-find","title":"Finding files with <code>find</code>","text":"<p>Basic syntax for <code>find</code> is </p> <pre><code>find path expression\n</code></pre> <p>find</p> <ul> <li>Path specifies the starting directory for search. Expressions are how we describe which files we want to <code>find</code> to return</li> <li> <p>Unlike <code>ls</code>, <code>find</code>is recursive (it will search through the directory structure). In fact, running <code>find</code> on a directory (without other arguments) is a quick way to see it\u2019s structure, e.g., <pre><code>find /nesi/project/nesi02659/| head\n</code></pre> <pre><code>find: /nesi/project/nesi02659/\n/nesi/project/nesi02659/.jupyter\n/nesi/project/nesi02659/.jupyter/share\n/nesi/project/nesi02659/.jupyter/share/jupyter\n/nesi/project/nesi02659/.jupyter/share/jupyter/nbconvert\n/nesi/project/nesi02659/.jupyter/share/jupyter/nbconvert/templates\n\u2018/nesi/project/nesi02659/.jupyter/share/jupyter/nbconvert/templates\u2019: Permission denied\n/nesi/project/nesi02659/.jupyter/share/jupyter/kernels\n/nesi/project/nesi02659/.jupyter/share/jupyter/kernels/sismonr\n/nesi/project/nesi02659/.jupyter/share/jupyter/kernels/sismonr/kernel.json\n/nesi/project/nesi02659/.jupyter/share/jupyter/kernels/sismonr/logo-64x64.png\n</code></pre></p> </li> <li> <p>Try the same command with <code>-maxdepth 1</code> .i.e. <pre><code>find /nesi/project/nesi02659/ -maxdepth 1 | head\n</code></pre></p> </li> </ul> Exercise 6.1 <ol> <li> <p>Create a small directory system as below in your current working directory</p> <pre><code>mkdir -p genome-project/{data/raw,scripts,results}\n</code></pre> <pre><code>touch genome-project/data/raw/bird{A,B,C}_R{1,2}.fastq\n</code></pre> </li> <li> <p>Run <code>find genome-project</code> and examine the output</p> Output <pre><code>genome-project/\ngenome-project/results\ngenome-project/data\ngenome-project/data/raw\ngenome-project/data/raw/birdB_R1.fastq\ngenome-project/data/raw/birdA_R1.fastq\ngenome-project/data/raw/birdA_R2.fastq\ngenome-project/data/raw/birdC_R1.fastq\ngenome-project/data/raw/birdB_R2.fastq\ngenome-project/data/raw/birdC_R2.fastq\ngenome-project/scripts\n</code></pre> </li> <li> <p>Use find to print the names of all files matching the pattern \u201cbirdB*fastq\u201d (e.g., FASTQ files from sample \u201cB\u201d, both read pairs): </p> <pre><code>find genome-project/data/raw/ -name \"birdB*fastq\"\n</code></pre> </li> <li> <p>This gives similar results to <code>ls birdB*fastq</code>, as we\u2019d expect. The primary difference is that find reports results separated by new lines and, by default, <code>find</code> is recursive. Because we only want to return <code>fastq</code> files (and not directories with that matching name), we might want to limit our results using the <code>-type</code> option: There are numerous different types you can search for; the most commonly used are <code>f</code>for files, <code>d</code> for directories, and <code>l</code> for links.</p> <pre><code>find genome-project/data/raw/ -name \"birdB*fastq\" -type f\n</code></pre> </li> <li> <p>By default, <code>find</code> connects different parts of an expression with logical AND. The find command in this case returns results where the name matches \u201cbirdB*fastq\u201d and is a file (type \u201cf \u201d). <code>find</code> also allows explicitly connecting different parts of an expression with different operators. If we want to get the names of all <code>fastq</code> files from samples A or C, we\u2019ll use the operator -or to chain expressions:</p> <pre><code>find genome-project/data/raw/ -name \"birdA*fastq\" -or -name \"birdC*fastq\" -type f\n</code></pre> </li> <li> <p>Another way to select these files is with negation: Some bash versions will accept <code>\"!\"</code> as the flag for this where others will require <code>-not</code> </p> <pre><code>find genome-project/data/raw/ -type f -not -name \"birdB*fastq\"\n</code></pre> </li> <li> <p>Suppose you were sharing this project folder with a colleague and a file named birdB_R1-temp.fastq was created by them in genome-project/data/raw but you want to ignore it in your file querying:</p> <pre><code>find genome-project/data/raw/ -type f -not -name \"birdB*fastq\" -and -not -name \"*-temp*\"\n</code></pre> </li> </ol>"},{"location":"6_automate_fileprocessing_find_xargs/#finds-exec-running-commands-on-finds-results","title":"<code>find</code>s <code>-exec</code>: Running Commands on find\u2019s Results","text":"<p><code>find</code>\u2019s real strength in bioinformatics is that it allows you to run commands on every file that is returned by find, using -<code>exec</code> option.</p> <p>Continuing from our last example, suppose that a collaborator created numerous temporary files. Let\u2019s emulate this (in the genome-project/data/raw/): (then <code>ls</code> ensure the <code>-temp.fastq</code> files were created)</p> <p>code</p> <pre><code>touch genome-project/data/raw/bird{A,C}_R{1,2}-temp.fastq\n</code></pre> <p>Although we can delete these files with <code>rm *-temp.fastq</code>, using <code>rm</code> with a wildcard in a directory filled with important data files is too risky. Using <code>find</code>\u2019s <code>-exec</code> is a much safer way to delete these files.</p> <p>For example, let\u2019s use <code>find -exec</code> and <code>rm</code> to delete these temporary files:</p> <p>code</p> <pre><code>find genome-project/data/raw/ -name \"*-temp.fastq\" -exec rm {} \\;\n</code></pre> <p>Notice the (required!) semicolumn and curly brackets at the end of the command! . In one line, we\u2019re able to pragmatically identify and execute a command on files that match a certain pattern. With <code>find</code> and <code>-exec</code>, a daunting task like processing a directory of 100,000 text files with a program is simple.</p> <p>In general, <code>find -exec</code> is most appropriate for quick, simple tasks (like deleting files, changing permissions, etc.). For larger tasks, <code>xargs</code> is a better choice.</p>"},{"location":"6_automate_fileprocessing_find_xargs/#xargs","title":"<code>xargs</code>","text":"<p><code>xargs</code> reads data from standard input (stdin) and executes the command (supplied to it as an argument) one or more times based on the input read. Any spaces, tabs, and newlines in the input are treated as delimiters, while blank lines are ignored. If no command is specified, <code>xargs</code> executes <code>echo</code>. (Notice, that echo by itself does not read from standard input!)</p> <p><code>xargs</code> allows us to take input from standard in, and use this input as arguments to another program, which allows us to build commands programmatically. Using <code>find</code> with <code>xargs</code> is much like <code>find -exec</code>, but with some added advantages that make <code>xargs</code> a better choice for larger tasks.</p> <p>Let\u2019s re-create our <code>-temp.fastq</code> files: .i.e Make sure to run <code>ls</code> after the <code>touch</code> command to verify the files were created. </p> <p>code</p> <pre><code>touch genome-project/data/raw/bird{A,C}_R{1,2}-temp.fastq\n</code></pre> <p><code>xargs</code> works by taking input from standard in and splitting it by spaces, tabs, and newlines into arguments. Then, these arguments are passed to the command supplied. For example, to emulate the behavior of <code>find -exec</code> with <code>rm</code>, we use <code>xargs</code> with <code>rm</code>:</p> <p>code</p> <pre><code>find genome-project/data/raw -name \"*-temp.fastq\" | xargs rm\n</code></pre> <p>One big benefit of <code>xargs</code> is that it separates the process that specifies the files to operate on (<code>find</code>) from applying a command to these files (through <code>xargs</code>). If we wanted to inspect a long list of files find returns before running <code>rm</code> on all files in this list, we could use:</p> <p>code</p> <p><pre><code>touch genome-project/data/raw/bird{A,C}_R{1,2}-temp.fastq\n</code></pre> <pre><code>find genome-project/data/raw/ -name \"*-temp.fastq\" &gt; ohno_filestodelete.txt\n</code></pre> <pre><code>cat ohno_filestodelete.txt\n</code></pre> <pre><code>cat ohno_filestodelete.txt  | xargs rm\n</code></pre></p> <p>Using <code>xargs</code> with Replacement Strings to Apply Commands to Files</p> <p>In addition to adding arguments at the end of the command, <code>xargs</code> can place them in predefined positions. This is done with the <code>-I</code> option and a placeholder string ({}). Suppose an imaginary program <code>fastq_stat</code> takes an input file through the option \u2013in, gathers FASTQ statistics information, and then writes a summary to the file specified by the \u2013out option. We may want our output filenames to be paired with our input filenames and have corresponding names. We can tackle this with <code>find</code>, <code>xargs</code>, and <code>basename</code>:</p>"},{"location":"puzzles/","title":"Puzzles","text":"Info <ul> <li>Download data (and answers  \ud83d\ude0a)</li> </ul> <pre><code>wget -c puzzles_da.tar.gz https://github.com/GenomicsAotearoa/shell-for-bioinformatics/releases/download/v2.0/puzzles_da.tar.gz -O - | tar -xz\n</code></pre> <ul> <li> <p>Review the content of the puzzles_da directory</p> <ul> <li>There are two directories, data and answers</li> <li>Each filename has a unique four character id which corresponds to the puzzle/question (Described below)</li> </ul> </li> <li> <p>We recommend appending the solution to a file and compare the content of it with the expected output in answers directory</p> </li> <li> <p>How to compare your solution with the provided solution  ( introducing two new commands <code>cmp</code> and <code>printf</code>)</p> </li> </ul> <pre><code>#!/bin/bash\n\nif cmp --silent -- \"provided_answer.txt\" \"my_answer.txt\"; then\n    printf \"\\U1F60A SUCCESS \\n\"\nelse \n    printf \"\\U1F97A Almost there...Try again please\\n\" \nfi\n</code></pre> <ul> <li><code>cmp</code> compare two files </li> <li><code>--silent</code> Output nothing; yield exit status only.</li> <li><code>printf</code> format and print data</li> <li><code>if</code> <code>else</code> <code>fi</code> are control statements. This will be covered in depth during the *Advanced shell for Bioinformatics\" workshop</li> </ul> <p></p> <p>Few additional bash commands to help with puzzles</p> <ol> <li><code>tr</code> - translating or deleting characters. It supports a range of transformations including uppercase to lowercase, squeezing repeating characters, deleting specific characters, and basic find and replace. It can be used with UNIX pipes to support more complex translation<ul> <li>Let's say we want to replace upper case <code>B</code> in <code>AABBCC</code> ( standard_input) with lower case <code>b</code>, we can use <code>standard_input | tr B b</code></li> </ul> </li> <li><code>rev</code> - reverse the lines characterwise. This utility basically reverses the order of the characters in each line by copying the specified files to the standard output. If no files are specified, then the standard input will read  <ul> <li>Suppose the standard input is \"Humpty Dumpty sat on a wall\", <code>rev standard_input</code> will print \"llaw a no tas ytpmuD ytpmuH\"</li> </ul> </li> </ol> Transcribing DNA into RNA (tdir) <p>An RNA string is a string formed from the alphabet containing 'A', 'C', 'G', and 'U'.</p> <p>Given a DNA string \\(t\\) corresponding to a coding strand, its transcribed RNA string \\(u\\) is formed by replacing all occurrences of 'T' in \\(t\\) with 'U' in \\(u\\)</p> <p>Given: A DNA string \\(t\\) having length at most 1000 nt.</p> <p>Return: The transcribed RNA string of \\(t\\)</p> Solution bashRPythonJuliaC++ <p>There are multiple ways to do this</p> <p><pre><code>sed 's/T/U/g' tdir_data.txt\n</code></pre> <pre><code>awk '{gsub(/T/,\"U\");print}' tdir_data.txt\n</code></pre> <pre><code>cat tdir_data.txt | tr T U\n</code></pre></p> <pre><code>gsub(\"T\",\"U\",readLines(\"tdir_data.txt\"))\n</code></pre> <pre><code>with open('tdir_data.txt', 'r') as f1:\n        dna = f1.read()\n        rna = dna.replace(\"T\", \"U\")\n        print(rna)\n</code></pre> <pre><code>seq = open(\"tdir_data.txt\") do file\n    read(file, String)\nend\n\nseq = replace(seq, \"T\" =&gt; \"U\")\nprintln(seq)\n</code></pre> <pre><code>// to compile, save the code to tdir.cpp:\n// g++ tdir.cpp -o tdir_cpp\n// to run:\n// cat tdir_data.txt | tdir_cpp &gt; answer\n\n#include &lt;iostream&gt;\nusing namespace std;            \n\nint main()\n{\n    char nucleotide;            \n\n    while (cin &gt;&gt;nucleotide) {\n        if (nucleotide == 'T') cout &lt;&lt;'U';\n        else cout &lt;&lt;nucleotide;\n    }\n    return 0;\n}\n</code></pre> Complementing a Strand of DNA (csod) <p>In DNA strings, symbols 'A' and 'T' are complements of each other, as are 'C' and 'G'.</p> <p>The reverse complement of a DNA string \\(s\\) is the string \\(s^{c}\\) formed by reversing the symbols of \\(s\\) then taking the complement of each symbol (e.g., the reverse complement of \"GTCA\" is \"TGAC\").</p> <p>Given: A DNA string s of length at most 1000 bp.</p> <p>Return: The reverse complement \\(s^{c}\\) of \\(s\\).</p> Solution bashPythonRJuliaPerlC++ <p>There are multiple ways to do this  <pre><code>rev csod_data.txt | tr ATCG TAGC\n</code></pre> <pre><code>rev csod_data.txt | sed 'y/ATCG/TAGC/'\n</code></pre> <pre><code>cat csod_data.txt | tr 'ACGT' 'TGCA' | rev\n</code></pre></p> <ul> <li> <p>For multi-line sequences <pre><code>tr -d \"\\n\" &lt; data.txt| rev  | tr ATCG TAGC\n</code></pre></p> </li> <li> <p>For FASTA files <pre><code>grep -v \"^&gt;\" r.fasta | tr -d \"\\n\"  | rev  | tr ATCG TAGC\n</code></pre></p> </li> </ul> <pre><code>for N in open(\"csod_data.txt\",\"r\").read()[::-1]:\n    for pair in [\"GC\",\"AT\"]:\n        if N in pair: print(\"\".join(set(N)^set(pair)),end=\"\")\n</code></pre> <pre><code>library(biostrings)\nreverseComplement(DNAString(readLines(\"tdir_data.txt\")))\n</code></pre> <pre><code>f = read(open(\"csod_data.txt\"), String)\n\ndict = Dict(\"A\"=&gt;\"T\", \"C\"=&gt;\"G\", \"T\"=&gt;\"A\", \"G\"=&gt;\"C\")\n\nfor i in reverse(f[1:end-1])\n    print(dict[string(i)])\nend\n\nprintln()\n</code></pre> <pre><code>$filename = 'csod_data.txt';\nopen(FILEN, $filename);            \n\n$dna = &lt;FILEN&gt;;            \n\n$rev = reverse $dna;\n$rev =~ tr/ATCG/TAGC/;            \n\nprint $rev;            \n\nexit;\n</code></pre> <pre><code>// to compile, save the code as csod.cpp\n// g++ csod.cpp -o csod_cpp\n// to run:\n// cat csod_data.txt | ./csod_cpp &gt; answer\n\n#include &lt;iostream&gt;\n#include &lt;vector&gt;            \n\nchar complement(const char c)\n{\n    switch(c) {\n        case 'A': return 'T';\n        case 'T': return 'A';\n        case 'C': return 'G';\n        case 'G': return 'C';\n    }\n    return 'X';\n}            \n\nint main()\n{\n    using std::cin;\n    using std::cout;\n    using std::vector;            \n\n    char nucleotide;\n    vector&lt;char&gt; DNAstring;            \n\n    while (cin &gt;&gt;nucleotide) {\n        DNAstring.push_back(nucleotide);\n    }\n    for (int pos = DNAstring.size()-1; pos &gt;= 0; --pos) {\n        cout &lt;&lt;complement(DNAstring[pos]);\n    }            \n\n    return 0;\n}\n</code></pre> Counting DNA Nucleotides (dnct) <p>A string is simply an ordered collection of symbols selected from some alphabet and formed into a word; the length of a string is the number of symbols that it contains.</p> <p>An example of a length 21 DNA string (whose alphabet contains the symbols 'A', 'C', 'G', and 'T') is \"ATGCTTCAGAAAGGTCTTACG.\"</p> <p>Given: A DNA string \\(s\\) of length at most 1000 nt.</p> <p>Return: Four integers (separated by spaces) counting the respective number of times that the symbols 'A', 'C', 'G', and 'T' occur in \\(s\\)</p> Solution bashPythonRRust <pre><code>awk '{a=gsub(\"A\",\"\");c=gsub(\"C\",\"\");g=gsub(\"G\",\"\");t=gsub(\"T\",\"\")} END {print a,c,g,t}' /path/to/file x=$(cat dnct_data.txt); for i in A C G T; do y=${x//[^$i]}; echo -n \"${#y} \"; done\n</code></pre> <ul> <li>A bit more simpler solution in <code>bash</code> <pre><code>for i in A C G T;do grep -o $i dnct_data.txt | wc -l; done\n</code></pre></li> </ul> <pre><code>with open('dnct_data.txt') as file:\n    dataset = file.read()\n\nprint(dataset.count('A'), dataset.count('C'), dataset.count('G'), dataset.count('T'))\n</code></pre> <pre><code>library(biostrings)\nletterFrequency(  DNAString( readLines(\"dnct_data.txt\")), letters = c(\"A\", \"C\",\"G\", \"T\"))\n</code></pre> <pre><code>// to compile, save the code as dnct.rs\n// rustc dnct.rs            \n\nuse std::fs::File;\nuse std::io::{Read, Write};            \n\nfn dna_parse(in_str: &amp;str) -&gt; (i32, i32, i32, i32) {\n    let mut a_count = 0;\n    let mut c_count = 0;\n    let mut g_count = 0;\n    let mut t_count = 0;\n    for symbol in in_str.chars() {\n        match symbol {\n            'A' | 'a' =&gt; a_count += 1,\n            'C' | 'c' =&gt; c_count += 1,\n            'G' | 'g' =&gt; g_count += 1,\n            'T' | 't' =&gt; t_count += 1,\n            '\\r' | '\\n' =&gt; (),\n            x =&gt; println!(\"Unknown DNA symbol: {}\", x.escape_debug())\n        };\n    }\n    (a_count, c_count, g_count, t_count)\n}            \n\nfn main() -&gt; std::io::Result&lt;()&gt; {\n    // Get the input data\n    let mut in_file = File::open(\"dnct_data.txt\")?;\n    let mut contents = String::new();\n    in_file.read_to_string(&amp;mut contents)?;            \n\n    // Process the data\n    let (a, c, g, t) = dna_parse(&amp;contents);\n    let out_str = format!(\"{} {} {} {}\", a, c, g, t);            \n\n    // Export the data\n    let mut out_file = File::create(\"dnct_answer.txt\")?;\n    out_file.write_all(out_str.as_bytes())?;\n    Ok(())\n}\n</code></pre> Maximum Matchings and RNA Secondary Structures (mmrs) <p>Figure</p> <p></p> <p>The graph theoretical analogue of the quandary stated in the introduction above is that if we have an RNA string \\(s\\) that does not have the same number of occurrences of 'C' as 'G' and the same number of occurrences of 'A' as 'U', then the bonding graph of \\(s\\) cannot possibly possess a perfect matching among its basepair edges. For example, see Figure 1; in fact, most bonding graphs will not contain a perfect matching.</p> <p>In light of this fact, we define a maximum matching in a graph as a matching containing as many edges as possible. See Figure 2 for three maximum matchings in graphs.</p> <p>A maximum matching of basepair edges will correspond to a way of forming as many base pairs as possible in an RNA string, as shown in Figure 3.</p> <p>Given: An RNA string \\(s\\) of length at most 100.</p> <p>Return: The total possible number of maximum matchings of basepair edges in the bonding graph of \\(s\\)</p> Solution bashPython <pre><code>#!/bin/bash\n\ndna=\"$(cat mmrs_data.txt | tail -n +2 | tr -d '\\n')\"\n\ncount () {\n    echo -n \"${dna//[^$1]/}\" | wc -c\n}\n\nmatches () {\nlocal n1=$(count $1)\nlocal n2=$(count $2)\nif test $n2 -gt $n1; then\n    local tmp=$n1\n    n1=$n2\n    n2=$tmp\nfi\nseq -s \\* $((n1 - n2 + 1)) $n1\n}\n\necho \"$(matches A U) * $(matches C G)\" | bc \n</code></pre> <pre><code>from Bio import SeqIO\nfrom scipy.special import perm\n\nwith open('mmrs_data.txt', encoding='utf-8') as handle:\n    rna = SeqIO.read(handle, 'fasta').seq\n\nau, cg = (rna.count('A'), rna.count('U')), (rna.count('C'), rna.count('G'))\nprint(perm(max(au), min(au), exact=True) * perm(max(cg), min(cg), exact=True))\n</code></pre> Computing GC Content (cgcc) <p>The GC-content of a DNA string is given by the percentage of symbols in the string that are 'C' or 'G'. For example, the GC-content of \"AGCTATAG\" is 37.5%. Note that the reverse complement of any DNA string has the same GC-content.</p> <p>DNA strings must be labeled when they are consolidated into a database. A commonly used method of string labeling is called FASTA format. In this format, the string is introduced by a line that begins with '&gt;', followed by some labeling information. Subsequent lines contain the string itself; the first line to begin with '&gt;' indicates the label of the next string.</p> <p>In our implementation, a string in FASTA format will be labeled by the ID \"GAotearoa_xxxx\", where \"xxxx\" denotes a four-digit code between 0000 and 9999.</p> <p>Given: At most 10 DNA strings in FASTA format (of length at most 1 kbp each).</p> <p>Return: The ID of the string having the highest GC-content, followed by the GC-content of that string. Rosalind allows for a default error of 0.001 in all decimal answers unless otherwise stated; please see the note on absolute error below.</p> Solution bashPerlR (with seqinr)PythonPython (\ud83e\udd13 version) <pre><code>#/bin/bash\n\nawk -v RS=\"&gt;\" -v FS=\"\\n\" 'BEGIN {max_id=\"\"; max_gc=\"\";} \\\n$0 !=\"\" { \\\ngc=0; \\\nl=0; \\\nfor(i=2;i&lt;=NF;i++) { \\\n    gc += gsub(/[GC]/, \".\", $i); \\\n    l += length($i);} \\\n    if(max_gc &lt; gc/l*100) {max_id=$1; max_gc=gc/l*100} \\\n} \\\nEND {print max_id\"\\n\"max_gc;}' cgcc_data.txt\n</code></pre> <pre><code>perl -ne 'BEGIN{$/=\"\\n&gt;\";$gc=-1}chomp;s/^&gt;//;($id,$seq)=(split(/\\n/,$_,2));$seq=~s/\\n//g;$cgc=100*(($seq=~y/GC//)/length($seq));if($cgc&gt;$gc){$gc=$cgc;$gcid=$id};END{print \"$gcid\\n$gc\\n\"}' data/cgcc_data.txt\n</code></pre> <pre><code>library(seqinr)\nfasta &lt;- read.fasta(\"cgcc_data.txt\")\ngc_content &lt;- apply(matrix(names(fasta)), 1, function(x){GC(fasta[[x]])})\nmost_gc &lt;- which(gc_content==max(gc_content))\n\n#Result\nrbind(names(fasta)[most_gc], paste(signif(gc_content[most_gc], 4) * 100, \"%\", sep=\"\"))\n</code></pre> <pre><code>from Bio import SeqIO\nfrom Bio.Seq import Seq\nfrom Bio.Alphabet import IUPAC\nfrom Bio.SeqUtils import GC\nmax_seq = Seq('tata', IUPAC.unambiguous_dna)\nmax_gc = ()\nfor seq_record in SeqIO.parse('cgcc_data.txt','fasta'):\n    if GC(seq_record.seq) &gt; GC(max_seq):\n        max_id = seq_record.id\n        max_seq = seq_record.seq\n        max_gc = GC(seq_record.seq)\nprint(max_id)\nprint (max_gc)\n</code></pre> <pre><code>from Bio import SeqIO\nfrom Bio.SeqUtils import GC\n\ninput_file = 'cgcc_data.txt'\nseq_dict = {\n    seq_record.id: GC(seq_record.seq)\n    for seq_record in SeqIO.parse(input_file, \"fasta\")\n}\n\nmax_gc = max(seq_dict, key=seq_dict.get) # find key of the dicionary's max value\nprint(max_gc)\nprint(seq_dict[max_gc])\n</code></pre> Counting Disease Carriers (cdcr) <p>To model the Hardy-Weinberg principle, assume that we have a population of \\(N\\) diploid individuals. If an allele is in genetic equilibrium, then because mating is random, we may view the 2\\(N\\) chromosomes as receiving their alleles uniformly. In other words, if there are m dominant alleles, then the probability of a selected chromosome exhibiting the dominant allele is simply \\(p=m/2N\\)</p> <p>Because the first assumption of genetic equilibrium states that the population is so large as to be ignored, we will assume that \\(N\\) is infinite, so that we only need to concern ourselves with the value of \\(p\\)</p> <p>Given: An array \\(A\\) for which \\(A[k]\\) represents the proportion of homozygous recessive individuals for the \\(k\\)-th Mendelian factor in a diploid population. Assume that the population is in genetic equilibrium for all factors.</p> <p>Return: An array \\(B\\) having the same length as \\(A\\) in which \\(B[k]\\) represents the probability that a randomly selected individual carries at least one copy of the recessive allele for the \\(k\\)-th factor.</p> Solution bashR <pre><code>cat cdcr_data.txt | tr \\  '\\n' | sed 's/.*/2 * sqrt(&amp;) - &amp;/' | bc -l\n</code></pre> <pre><code>nums &lt;- readLines(\"cdcr_data.txt\")                \nnums &lt;- as.numeric(unlist(strsplit(nums, \" \")))\n2 * sqrt(nums) - nums\n</code></pre>"},{"location":"supplementary%20/supplementary_1/","title":"Supp - 1. UNIX, Linux  &amp; UNIX Shell","text":"<p>Lesson Objectives</p> <ul> <li>Quick overview on UNIX operating system and it's importance</li> <li>Differences/similarities between UNIX vs. Linux</li> <li>What is a shell and the importance of UNIX shell/s for Bioinformatics (or for Scientific computing in general)</li> <li>Types of Shell and intro to Bash Shell (we will be using the latter throughout the workshop)</li> </ul> <p></p>"},{"location":"supplementary%20/supplementary_1/#the-unix-operating-system","title":"The UNIX operating system","text":"<p>Unix is a multi-user operating system which allows more than one person to use the computer resources at a time. It was originally designed as a time-sharing system to serve several users simultaneously. Unix allows direct communication with the computer via a terminal, hence being very interactive and giving the user direct control over the computer resources. Unix also gives users the ability to share data and programs among one another.</p> <p>Unix is a generic operating system which takes full advantage of all available hardware such as 32-bit processor chips, expanded memory, and large, fast hard drives. Since Unix is written in a machine-independent language (C/C++) it is portable to many different types of machines including PC's. Therefore, Unix can be adapted to meet special requirements. The UNIX operating system is made up of three parts; the kernel, the shell and the programs.</p> <p>On Unix philosophy</p> <p>\u201cAlthough that philosophy can\u2019t be written down in a single sentence, as its heart is the idea that the power of a system comes more from the relationships among programs than from the programs themselves. Many UNIX programs do quite trivial things in isolation, but, combined with other programs, become general and useful tools.\u201d \u2013 Brian Kernighan &amp; Rob Pike</p> <p>The UNIX operating system is made up of three parts; the kernel, the shell and the programs</p> <p>Kernel \u2212 The kernel is the heart of the operating system. It interacts with the hardware and most of the tasks like memory management, task scheduling and file management.</p> <p>Shell \u2212 The shell is the utility that processes your requests (acts as an interface between the user and the kernel). When you type in a command at your terminal, the shell interprets (operating as in interpreter) the command and calls the program that you want. The shell uses standard syntax for all commands. The shell recognizes a limited set of commands, and you must give commands to the shell in a way that it understands: Each shell command consists of a command name, followed by command options (if any are desired) and command arguments (if any are desired). The command name, options, and arguments, are separated by blank space. </p> <ul> <li>An interpreter operates in a simple loop: It accepts a command, interprets the command, executes the command, and then waits for another command. The shell displays a \"prompt,\" to notify you that it is ready to accept your command.  </li> </ul>"},{"location":"supplementary%20/supplementary_1/#unix-vs-linux","title":"UNIX vs. Linux","text":"<p>Linux is not Unix, but it is a \"Unix-like\" operating system. Linux system is derived from Unix and it is a continuation of the basis of Unix design. Linux distributions are the most famous and healthiest example of the direct Unix derivatives. BSD (Berkley Software Distribution) is also an example of a Unix derivative.</p> Unix-like &amp; a bit more on Linux <p>A Unix-like OS (also called as UN*X or *nix) is the one that works in a way similar to Unix systems, however, it is not necessary that they conform to Single UNIX Specification (SUS) or similar POSIX (Portable Operating System Interface) standard.</p> <p>SUS is a standard which is required to be met for any OS (Operating System) to qualify for using \u2018UNIX\u2019 trademark. This trademark is granted by \u2018The Open Group\u2019</p> <p>Some examples of currently registered UNIX systems include macOS, Solaris, and AIX. If we consider the POSIX system, then Linux can be regarded as Unix-like OS.</p> <p>Linux is just the kernel and not the complete OS. This Linux kernel is generally packaged in Linux distributions which thereby makes it a complete OS.</p> <p>Linux distribution (also called a distro) is an operating system that is created from a collection of software built upon the Linux Kernel and is a package management system. A standard Linux distribution consists of a Linux kernel, GNU system, GNU utilities, libraries, compiler, additional software, documentation, a window system, window manager and a desktop environment. Most of the software included in a Linux distribution is free and open source. They may include some proprietary software like binary blobs which are essential for a few device drivers.</p>"},{"location":"supplementary%20/supplementary_1/#unix-shell-for-bioinformatics","title":"UNIX Shell for Bioinformatics","text":"<p>A shell is a computer program that presents a command line interface which allows you to control your computer using commands entered with a keyboard instead of controlling graphical user interfaces (GUIs) with a mouse/keyboard/touchscreen combination.</p> <p>There are many reasons to learn about the shell:</p> <ul> <li>Many bioinformatics tools can only be used through a command line interface. Many more have features and parameter options which are not available in the GUI. BLAST is an example. Many of the advanced functions are only accessible to users who know how to use a shell.</li> <li>The shell makes your work less boring. In bioinformatics you often need to repeat tasks with a large number of files. With the shell, you can automate those repetitive tasks and leave you free to do more exciting things.</li> <li>The shell makes your work less error-prone. When humans do the same thing a hundred different times (or even ten times), they\u2019re likely to make a mistake. Your computer can do the same thing a thousand times with no mistakes.</li> <li>The shell makes your work more reproducible. When you carry out your work in the command-line (rather than a GUI), your computer keeps a record of every step that you\u2019ve carried out which you can use to re-do your work when you need to. It also gives you a way to communicate unambiguously what you\u2019ve done, so that others can inspect or apply your process to new data.</li> <li>Many bioinformatic tasks require large amounts of computing power and can\u2019t realistically be run on your own machine. These tasks are best performed using remote computers or cloud computing, which can only be accessed through a shell.</li> </ul>"},{"location":"supplementary%20/supplementary_1/#different-types-of-shells","title":"Different Types of Shells","text":"<p>Being able to interact with the kernel makes shells a powerful tool. Without the ability to interact with the kernel, a user cannot access the utilities offered by their machine\u2019s operating system.</p> <p>Let\u2019s take a look at some of  the major shells that are available for the Linux environment</p> <p>Types of Shells</p> Bourne Shell (sh)C Shell (csh)Korn Shell (ksh)Z Shell (zsh)Fish Shell (fish) <p>Developed at AT&amp;T Bell Labs by Steve Bourne, the Bourne shell is regarded as the first UNIX shell ever. It is denoted as sh. It gained popularity due to its compact nature and high speeds of operation.</p> <p>The C shell was created at the University of California by Bill Joy. It is denoted as csh. It was developed to include useful programming features like in-built support for arithmetic operations and a syntax similar to the C programming language.</p> <p>Further, it incorporated command history which was missing in different types of shells in Linux like the Bourne shell. Another prominent feature of a C shell is \u201caliases\u201d.</p> <p>The complete path-name for the C shell is <code>/bin/csh</code>. By default, it uses the prompt <code>hostname#</code> for the root user and <code>hostname%</code> for the non-root users.</p> <p>The Korn shell was developed at AT&amp;T Bell Labs by David Korn, to improve the Bourne shell. It is denoted as ksh. The Korn shell is essentially a superset of the Bourne shell.</p> <p>Besides supporting everything that would be supported by the Bourne shell, it provides users with new functionalities. It allows in-built support for arithmetic operations while offering interactive features which are similar to the C shell.</p> <p>The Korn shell runs scripts made for the Bourne shell, while offering string, array and function manipulation similar to the C programming language. It also supports scripts which were written for the C shell. Further, it is faster than most different types of shells. </p> <p>zsh is a shell designed for interactive use, although it is also a powerful scripting language. Many of the useful features of bash, ksh, and tcsh were incorporated into zsh; many original features were added.</p> <p>Fish is a fully-equipped command line shell (like bash or zsh) that is smart and user-friendly. Fish supports powerful features like syntax highlighting, autosuggestions, and tab completions that just work, with nothing to learn or configure.</p> <p>If you want to make your command line more productive, more useful, and more fun, without learning a bunch of arcane syntax and configuration options, then fish might be just what you\u2019re looking for!</p>"},{"location":"supplementary%20/supplementary_1/#type-of-shell-for-this-workshop-gnu-bourne-again-shell-bash","title":"Type of Shell for this workshop: GNU Bourne-Again shell (bash)","text":"<p>The GNU Bourne-Again shell was designed to be compatible with the Bourne shell. It incorporates useful features from different types of shells in Linux such as Korn shell and C shell.</p> <ul> <li>The shell's name bash is an acronym for \"Bourne Again Shell\", a pun on the name of the Bourne shell that it replaces and the notion of being \"born again\"</li> </ul> <p>First released in 1989, it has been used as the default login shell for most Linux distributions. Bash was also the default shell in all versions of Apple macOS prior to the 2019 release of macOS Catalina, which changed the default shell to zsh, although Bash remains available as an alternative shell</p> <p>The Bash command syntax is a superset of the Bourne shell command syntax. Bash supports brace expansion, command line completion (Programmable Completion), basic debugging and signal handling (using <code>trap</code>) among other features. Bash can execute the vast majority of Bourne shell scripts without modification, with the exception of Bourne shell scripts stumbling into fringe syntax behavior interpreted differently in Bash or attempting to run a system command matching a newer Bash builtin, etc. </p> <p>Back to homepage</p>"},{"location":"supplementary%20/supplementary_2/","title":"Supp - 2. Unix Shell Basics","text":"<p>Objectives \ud83d\ude36\u200d\ud83c\udf2b\ufe0f</p> <ul> <li>Navigate your file system using the command line.</li> <li>Quick recap on commands used in routine tasks such copy, move, remove.</li> </ul> <p>It is expected that you are already familiar with using the basics of the Unix Shell. As a quick refresher, some frequently used commands are listed below.</p> <p>For more information about a command, use the Unix <code>man</code> (manual) command. For example, to get more information about the <code>mkdir</code> command, type:</p> <pre><code>man mkdir\n</code></pre> <p>Key commands for navigating around the filesystem are:</p> <ul> <li><code>ls</code> - list the contents of the current directory</li> <li><code>ls -l</code> - list the contents of the current directory in more detail</li> <li><code>pwd</code> - show the location of the current directory</li> <li><code>cd DIR</code> - change directory to directory DIR (DIR must be in your current directory - you should see its name when you type <code>ls</code> OR you need to specify either a full or relative path to DIR)</li> <li><code>cd -</code> - change back to the last directory you were in</li> <li><code>cd</code> (also <code>cd ~/</code>) - change to your home directory</li> <li><code>cd ..</code> - change to the directory one level above</li> </ul> <p>Other useful commands:</p> <ul> <li><code>mv</code> - move files or directories</li> <li><code>cp</code> - copy files or directories</li> <li><code>rm</code> - delete files or directories</li> <li><code>mkdir</code> - create a new directory</li> <li><code>cat</code> - concatenate and print text files to screen</li> <li><code>more</code> - show contents of text files on screen</li> <li><code>less</code> - cooler version of <code>more</code>. Allows searching (use <code>/</code>)</li> <li><code>tree</code> - tree view of directory structure</li> <li><code>head</code> - view lines from the start of a file</li> <li><code>tail</code> - view lines from the end of a file</li> <li><code>grep</code> - find patterns within files</li> </ul> <p>Back to homepage</p>"},{"location":"supplementary%20/supplementary_3/","title":"Supp - 3. Escaping and Special Characters","text":""},{"location":"supplementary%20/supplementary_3/#escaping","title":"Escaping","text":"<p>Escaping is a method of quoting single characters. The escape <code>(\\)</code> preceding a character tells the shell to interpret that character literally.</p> <p>Warning</p> <p>With certain commands and utilities, such as <code>echo</code> and <code>sed</code>, escaping a character may have the opposite effect - it can toggle on a special meaning for that character.</p> Special meanings of certain escaped characters Character Meaning <code>\\n</code> newline <code>\\r</code> return <code>\\t</code> tab <code>\\v</code> vertical tab <code>\\b</code> backspace <code>\\a</code> alert (beep or flash) <code>0xx</code> translates to the octal ASCII equivalent of 0nn, where nn is a string of digits"},{"location":"supplementary%20/supplementary_3/#special-characters","title":"Special Characters","text":"Special Characters Character Meaning <code>#</code> Comments. Lines beginning with a <code>#</code> (with the exception of <code>#!</code>) are comments and will not be executed. <code>;</code> Command separator [semicolon]. Permits putting two or more commands on the same line. <code>;;</code> Terminator in a case option [double semicolon]. <code>\"</code> \"partial quoting [double quote]. \"STRING\" preserves (from interpretation) most of the special characters within STRING <code>'</code> full quoting[single quote] 'STRING' preserves all special characters within STRING. This is a stronger form of quoting than \"STRING\" <code>,</code> comma operator. The comma operator [1] links together a series of arithmetic operations. All are evaluated, but only the last one is returned. <code>,, ,</code> Lowercase conversion in parameter substitution <code>\\</code> escape [backslash]. A quoting mechanism for single characters. <code>/</code> Filename path separator [forward slash]. Separates the components of a filename.This is also the division arithmetic operator. ` command substitution. The command construct makes available the output of command for assignment to a variable. This is also known as backquotes or backticks. <code>:</code> null command [colon]. This is the shell equivalent of a \"NOP\" (no op, a do-nothing operation). It may be considered a synonym for the shell builtin true <code>!</code> reverse (or negate) the sense of a test or exit status [bang]. The ! operator inverts the exit status of the command to which it is applied <code>?</code> test operator. Within certain expressions, the ? indicates a test for a condition. <code>$</code> Variable substitution (contents of a variable). <code>${}</code> Parameter substitution <code>$?</code> Exit status variable <code>$$</code> process ID variable <code>()</code> command group <code>{}</code> Block of code [curly brackets]. Also referred to as an inline group, this construct, in effect, creates an anonymous function (a function without a name). <code>{} \\;</code> pathname. Mostly used in <code>find</code> constructs. This is not a shell builtin. <code>&gt;|</code> force redirection (even if the noclobber option is set). This will forcibly overwrite an existing file. <code>||</code> OR logical operator. In a test construct, the <code>&amp;</code> Run job in background. A command followed by an <code>&amp;</code> will run in the background. <code>&amp;&amp;</code> AND logical operator. In a test construct, the &amp;&amp; operator causes a return of 0 (success) only if both the linked test conditions are true. <code>~+</code> current working directory. This corresponds to the <code>$PWD</code> internal variable. <code>~-</code> previous working directory. This corresponds to the <code>$OLDPWD</code> internal variable. <code>=~</code> regular expression match. This operator was introduced with version 3 of Bash. <code>^</code> beginning-of-line. In a regular expression, a \"^\" addresses the beginning of a line of text. <code>^, ^^</code> Uppercase conversion in parameter substitution"}]}